{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWnEM2KYcdsT"
   },
   "source": [
    "# **Assignment** - **2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "P1PhHGsa3MHA",
    "outputId": "ee9128f3-52e7-4f70-9729-58f489695320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries and tools\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.datasets import mnist\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import  fetch_20newsgroups\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zvv7q0vKczx4"
   },
   "source": [
    "## Ques - 1\n",
    "### **Word2Vec**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo8kMrwidl3d"
   },
   "source": [
    "*   20-newgroup dataset is a collection of newsgroups in 20 topics. Fetch 20-newsgroup dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "OKOqePML4Bv4",
    "outputId": "70c40865-696c-4a80-d8ab-777708dbaae2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# loading the 20newsgroup dataset\n",
    "# returns Bunch object with the following attribute:\n",
    "#     - data: list, length [n_samples]\n",
    "#     - target: array, shape [n_samples]\n",
    "#     - filenames: list, length [n_samples]\n",
    "#     - DESCR: a description of the dataset.\n",
    "#     - target_names: a list of categories of the returned data,\n",
    "#     - length [n_classes]. This depends on the `categories` parameter while calling.\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4Ou1t0LdzOx"
   },
   "source": [
    "*    Pre-process the dataset: Convert to lowercase, remove punctuations, symbols, and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkdvAhOGptKZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = []\n",
    "\n",
    "for i in range(len(newsgroups.data)):\n",
    "    # tokeninzing the data, removing non-alphanumeric characters\n",
    "    dt = tokenizer.tokenize(newsgroups.data[i])\n",
    "\n",
    "    # converting tokens to lowecase and ignoring stop-words\n",
    "    words.append([w.lower() for w in dt if w not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TogAHyIveNrp"
   },
   "source": [
    "*     Convert the words in the dataset to vectors of dimension 100 using Word2Vec. Ignore words whose frequency is less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DobbXGQsrkTi"
   },
   "outputs": [],
   "source": [
    "# converting words to vectors of size 100, ignoring words with frequency less than 10\n",
    "model = Word2Vec(words, min_count=10, size=100, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66_olQ31eT2y"
   },
   "source": [
    "*    Find the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "al6XzTbZw-Sn",
    "outputId": "ed129161-6f58-4c2d-b99d-395be6115a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is:  22848\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab = model.wv.vocab.keys()\n",
    "print(\"The size of the vocabulary is: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPWZR8ZmeXpz"
   },
   "source": [
    "*    Find the most similar words in the corpus to the word “car” along with their similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "JTMXgrWjxbJc",
    "outputId": "8459ec2a-5303-44bb-d4e3-2d5cca1aa489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar words to 'car': \n",
      "\t bike : 0.8241970539093018\n",
      "\t tires : 0.8129745721817017\n",
      "\t bikes : 0.8125572800636292\n",
      "\t dealer : 0.8096601963043213\n",
      "\t motorcycle : 0.7991973161697388\n",
      "\t cars : 0.7927560210227966\n",
      "\t tire : 0.7898772954940796\n",
      "\t odometer : 0.7797927856445312\n",
      "\t bmw : 0.7711659669876099\n",
      "\t owner : 0.7630698084831238\n"
     ]
    }
   ],
   "source": [
    "# top words in the corpus similar to 'car'\n",
    "sim_to_car = model.wv.similar_by_word('car', topn=10)\n",
    "print(\"Top 10 similar words to 'car': \")\n",
    "for pr in sim_to_car:\n",
    "    (x, y) = pr\n",
    "    print(\"\\t\", x, \":\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAhpelmoelF3"
   },
   "source": [
    "*    Find the top 5 words similar to the following operations:\n",
    "     *    girl + father - boy\n",
    "     *    sports - bat + ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "fOjdz3o2PBV9",
    "outputId": "65fc4435-5a5e-494e-f273-f55a4f65df5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'girl + father - boy':\n",
      "\t son : 0.8415806293487549\n",
      "\t mother : 0.83420330286026\n",
      "\t husband : 0.8079642057418823\n",
      "\t moses : 0.798603355884552\n",
      "\t female : 0.7950559854507446\n",
      "\n",
      "Words similar to 'sports + ball - bat':\n",
      "\t arena : 0.8422720432281494\n",
      "\t club : 0.7807680368423462\n",
      "\t wearing : 0.7646790742874146\n",
      "\t town : 0.760001540184021\n",
      "\t winter : 0.7594608664512634\n"
     ]
    }
   ],
   "source": [
    "# top 5 words similar to (girl + father - boy)\n",
    "sim_to_gf_b = model.wv.most_similar(positive=['girl', 'father'], negative=['boy'], topn=5)\n",
    "sim_to_sb_b = model.wv.most_similar(positive=['sports', 'ball'], negative=['bat'], topn=5)\n",
    "\n",
    "print(\"Words similar to 'girl + father - boy':\")\n",
    "for pr in sim_to_gf_b:\n",
    "    (x, y) = pr\n",
    "    print(\"\\t\", x, \":\", y)\n",
    "    \n",
    "print(\"\\nWords similar to 'sports + ball - bat':\")\n",
    "for pr in sim_to_sb_b:\n",
    "    (x, y) = pr\n",
    "    print(\"\\t\", x, \":\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5n83n0VeuLx"
   },
   "source": [
    "*    Create a TSNE plot for the top 20 words similar to each of the words {‘baseball’, ‘software’,\n",
    "‘police’, ‘government’, ‘circuit’, ‘car’}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "hW_mTJ_HTcYk",
    "outputId": "9a937dbc-9dac-4630-aafa-281fcbdd6ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to baseball : ['hockey', 'stats', 'football', 'nhl', 'playoff']\n",
      "Words similar to software : ['platform', 'vendor', 'packages', 'vendors', 'developer']\n",
      "Words similar to police : ['officers', 'officials', 'agents', 'cops', 'guard']\n",
      "Words similar to government : ['govt', 'regulation', 'federal', 'legitimate', 'agencies']\n",
      "Words similar to circuit : ['signal', 'signals', 'analog', 'transmitter', 'components']\n",
      "Words similar to car : ['bike', 'tires', 'motorcycle', 'bikes', 'dealer']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xdc1dX/wPHXhz0FFdwmaoJsEBBM\nUcRZmiMHJpojNTPTr371l6kZjsySTDHHN3OLucqFlqZCIIobFw4cqOACGQrIPr8/rtwAIQfXfZ6P\nh4+4n/u55zPI+/Z8zjnvtyKEQJIkSZJKo/WiT0CSJEl6eckgIUmSJJVJBglJkiSpTDJISJIkSWWS\nQUKSJEkqkwwSkiRJUplkkJAkSZLKJIOEJEmSVCYZJCRJkqQy6bzoEyiLhYWFsLKyetGnIUmS9Eo5\ncuRIkhDCUlPtvbRBwsrKisOHD7/o05AkSXqlKIpyRZPtycdNkiRJUplkkJAkSZLKJIOEJElSOfj4\n+Kgfjb/33nukpqa+4DPSrJd2TEKSJOlVs3379hd9ChonexKSJElFxMXF0bBhQ/z9/bG1taV79+5k\nZmaye/duXF1dcXR0ZODAgWRnZz/0WSsrK5KSkgBYsWIFTk5OODs707dvXwASExPp1q0bHh4eeHh4\nEBkZ+Vyv7WnIICFJklTCuXPnGDZsGGfOnKFChQrMmjWL/v37s3btWk6ePEleXh4LFiwo8/OnT59m\n2rRp7Nmzh+PHjzNnzhwARo4cyahRozh06BC//fYbgwYNel6X9NRkkJAkSSqhdu3aNG3aFIA+ffqw\ne/du6tati7W1NQD9+vUjPDy8zM/v2bOHHj16YGFhAUClSpUA2LVrF8OHD8fFxYVOnTpx9+5d0tPT\ngYd7HnFxcfj6+uLk5ESrVq24evUqAP3792fo0KG4u7tjbW1NSEgIoApMjRs3BrBTFOWEoigNNHEv\n5JiEJElvvG2XtjHn6BxuZtzELMOMrPysYu+bm5tz586dch+noKCAqKgoDAwMim0v7Hns27cPCwsL\nkpOT6devn/rPkiVLGDFiBJs2bQJUj8QOHjzIxYsXadmyJRcuXGDhwoWMHDmSPn36xADvANrlPmFk\nT0KSpDfctkvbCNgXwI2MGwgEtzNvk3g9kcANgQCsXr0ad3d34uLiuHDhAgArV66kRYsWZbbp6+vL\n+vXr1YElOTkZgLZt2zJ37lz1fnND5tJ2Q1vafNuGPIc8Dtw9AKh6Hvv376d3794A9O3bl71796o/\n17NnT7S0tGjQoAH16tXj7NmzNGnShOnTpwNUA+oIIe5r4v7IICFJ0httztE5D/Uc9Krp8UPQD9ja\n2pKSksKoUaNYunQpPXr0wNHRES0tLYYOHVpmm/b29kyYMIEWLVrg7OzM6NGjAQgKCuLw4cM4OTnx\nVoO3mPrjVG5k3AAgPTedgH0BbLu07ZHnrCjKQ6979+7Nli1bAAqA7Yqi+D7RjSjrWEKI8jeiKO2B\nOai6N78IIWaUsV83YAPgIYT415wb7u7uQqblkCTpWXNa7oTgn+/BnMQcrsy+gvU31pzod+KZHbft\nhrbqAJGVkMXVoKvU+6oetavWZo3vGvr370+PHj3o27cvy5YtY/PmzWzcuJH+/ftz+/ZtQkJCuHz5\nMi1atODChQtcv36dunXroqWldQQIA+KFELPLe57lHpNQFEUbmAe0AeKBQ4qibBFCxJTYzxQYCRwo\n7zElSZI0pZpxNfWXdcntz9LNjJvqnw1qGmD5viWXv71MnFYco31HM3fuXAYMGMDMmTOxtLRk6dKl\n6v3feustGjduzN27d1m4cCEGBgasW7eOlStXAtgBScB0TZxnuXsSiqI0AQKEEO0evP4SQAjxbYn9\nZgN/AWOBMbInIUnSy6BwTKLoIycDbQMC3gmgQ70Oz+y4RXsSRVU3rs7O7jvL/Fz//v3p2LEj3bt3\nL/V9RVGOCCHcNXWemhiTqAlcK/I6/sE2NUVRGgG1hRCPftgmSZL0HHWo14GAdwKoblwdBYXqxtWf\neYAAGNloJAbaxWc5GWgbMLLRyGd63Cf1zKfAKoqiBcwC+j/GvkOAIaDqTkmSJD0PHep1eOZBobRj\nAuqpt9WMqzGy0chHnseyZcs0cnxFUQKAdCFE4L/tp4kgkQDULvK61oNthUwBByDswYh8NWCLoiid\nSj5yEkL8DPwMqsdNGjg3SZKkl9bzCk6KougIIfKe5rOaeNx0CGigKEpdRVH0gF7AlsI3hRBpQggL\nIYSVEMIKiAIeChCSJElvgqCgIGxtbfH39yc7O5vWrVvj4uLC2rVrGTRoEDExMUydOhUbGxuaNWvG\nhx9+SGBgYLFss0lJSRRW7oyLi8Pb25tGjRrRqFEjAGMARVF8FEWJUBRlCxDzYNsERVHOK4qyF7B5\nnPMtd09CCJGnKMpwYAeqKbBLhBCnFUWZAhwWQmz59xYkSZLeHPPnz2fXrl3UqlWLqKgoAKKjowHw\n8/NT53U6fvw4ubm5NGrUCDc3tzLbq1KlCn/99RcGBgbExsZibW1d9Fl9I8BBCHFZURQ3VP+Id0H1\n3X8UOPKo89XImIQQYjuwvcS2SWXs66OJY0qSJL3sZs2axZIlSwAYNGgQZ8+e5dKlS7z77rv06dOH\nRYsWkZiYiIuLC7/99hsff/wxjRo1onPnzoSFhTF+/HhSU1P53//+R82aNbl//z4DBw7k2LFjXL9+\nnc2bN+Pj48O7777LqVOneDBb1UhRlOqo1q4VAFsVRfkUcAM2CiEyAR70MB5J5m6SJEl6Bo4cOcLS\npUs5cOAAQgg8PT1ZtWoVf/75J6GhoVhYWODp6UlgYCAhISFsu7SNE4knOBVzChM9E+b/PJ+D+w4y\nd+5czM3NCQ8PZ/HixbRu3ZopU6bwzjvvMHbsWLp3706FChUwNTXl2LFjVK1aFaA3qqGAa0BnwAhV\nkHhiMi2HJEmShmy7tI22G9ritNwJ/5/8cfBxwNjYGBMTEz744AMiIiLK/FzAvgCy87MxqG3A9ajr\n5NfJ50j6EUJCQjAyMsLKyorQ0FBmzJiBp6cnN2/eJCsri4SEBMzNzWnbti3btqlXGRwC3gWsAUch\nxD0gHOiiKIrhg8XN7z/ONckgIUmSpAElEwWm5aTxd/zfj5WLac7ROWTczSA3JReDmgYY1jMk7Xga\ng3sOxtHRETMzM8aMGcPt27cpKChgwIAB1KhRg6tXrzJx4kQiIyPZunUrZ8+eBSgQQoQDI4D7wDJF\nUT4SQhwF1gLHgT9QBZJHkkFCkiRJA0omCjS2Nib5cDKz9s8iIyODjRs34u3tzd27d3nnnXfw9/cn\nJyeHqKgoIkZHkLovldykXHISc6jWvRpaRlpY9rXkypUrvP322zRs2JCOHTtiZmbG1KlTiYuL49ix\nYzRo0ICpU6fSq1cvvvvuO4CriqIsBzYJIZyBX1ANYCOE+EYIYS2EaCaE6P2oNRIgg4QkSdJj69Kl\nC25ubtjb2/Pzzz8DsHjxYqytrdn7f3tJWJLA9ZXXAdCtpAtA6ADV+IOvry+urq6kpqZiZ2dHQkIC\nvXv35v79+3jP8iYzNhME3PrtFldmXUFRFGInxnL8+HHef/99IiIi1IWJnJycsLe356uvvirrVGsC\nxxVFOQb4oRrEfipy4FqSJOkxLVmyhEqVKnH//n08PDzo0KEDU6dO5ejRo/Tc2ZP9X+/HoLYq1caN\n1TcwtjFGR9GhhkEN1q5dS05ODoqisHPnTiZMmMCVK1dITk4m9qtYqn5QlbvRd6k9tDaGdQ25uewm\nBTEF1KpWi8qVK6vXUujq6nLy5Em2bt3KtGnTcHV1pXLlygQHBxc91bNCiNaKovQAvgY2KYqSJoRo\n/qTXLHsSkiRJjykoKAhnZ2e8vLy4du2auvhQpUqVGNV4FJUbV1bve+/EPVLCU7A0ssTAwIDk5GT6\n9u2Lqakp//nPf5gwYQJLly7FyMiIiL8i+KLdF+o6EZXzKpN7IhdTQ1O6du3K119/jampabFzadas\nGVFRURw7doxevXrx/fffl3bKk4B2Dx47dXqaa5Y9CUmSpDIULWuqF6fH/ZD7HNp/CCMjI3x8fGjY\nsCFnzpwBVCk23q37LltuqZYfiHxBzfdq8tOsn+hQrwNfffUVR46o1q7p6+urj6EoCnl5efi+5YuB\ntgFrOq7hxo0brGm/hpkzZ7Jt2zZGjhypLlxUKD4+Hj8/P27cuEFOTg5169Yt7RIiUQ1crwN+f5p7\nIHsSkiRJpSg5WykxOZGE/ARCb4Zy9uxZoqKiyMjI4O+//yYlJYW8vDz27dhHvsgHQL+aPimxKepq\nczdv3vzX45mampKfn69+nZGRQdWqVRk8eDCDBg3i6NGjxfb//PPPGT58OCdPnuR///sfWVlZJZtE\nCDEUmIgqv94RRVEqP7TTI8iehCRJUilKzlYycTQhOTQZv+Z+tHZvjZeXFzVr1mT8+PE0btyYSpUq\nccfwDjzI/l2tZzWu/nSVk+NO0lP0RP++PsOGDSvzeJUrV8bMzAw/Pz/at29PeHg4dnZ2GBkZYWBg\nwOrVq/n777/V+6elpVGzpqoqw/Lly0ttU1GU+kKIA8ABRVHeRRUs7jzJfZA9CUmSpFIUrRwHoKWr\nhdV/raj3TT02bdpEWFgYPj4+9O7dm9jYWCIjI0lPTcewriEAJvYmVOlcBYB8JZ9Jkybh6uqKubk5\nn332mbrdFi1aqJP12drasnbtWubNm0dwcDBGRkYUFBRgYmLy0OOkgIAAevTogZubGxYWFmVdxkxF\nUU4qinIK2IdqjcQT0UiN62dBVqaTJOlFetzKcWPGjGHXrl1kZWWRWS+TCj0rqAegy/rMs/QyVqaT\nJEl67Txu5bjAwECio6M5e/YsC35agKGO4SM/8yqRYxKSJEmleJrKcU9bbe5lJh83SZIkvUbk4yZJ\nkiTpuZFBQpIkSSqTDBKSJElSmWSQkCRJksokg4QkSZJUJo0ECUVR2iuKck5RlAuKoowr5f2hD1b9\nRSuKsldRFDtNHFeSJEl6tsodJBRF0Qbmoaqnagd8WEoQWC2EcBRCuADfA7PKe1xJkiTp2dNET6Ix\ncEEIcUkIkQOsAToX3UEIcbfIS2Pg5VycIUmSJBWjiRXXNYFrRV7HA54ld1IU5TNgNKAH+GrguJIk\nSdIz9twGroUQ84QQ9YEvUOU3f4iiKEMURTmsKMrhxMTE53VqkiRJUhk0ESQSUOUoL1TrwbayrAG6\nlPaGEOJnIYS7EMLd0tJSA6cmSZIklYcmgsQhoIGiKHUVRdEDegFbiu6gKEqDIi87ALEaOK4kSZL0\njJV7TEIIkacoynBgB6ANLBFCnFYUZQpwWAixBRiuKEprIBdIAfqV97iSJEnSs6eRVOFCiO3A9hLb\nJhX5+dVNpi5JkvQGkyuuJUmSpDLJICFJkiSVSQYJSZIkqUwySEiSJEllkkFCkiRJKpMMEpIkSVKZ\nZJCQJEmSyiSDhCRJklQmGSQkSZKkMskgIUmSJJVJBglJkiSpTDJISJIkSWWSQUKSJEkqkwwSkiRJ\nUplkkJAkSZLKJIOEJEmSVCYZJCRJkqQyySAhSZIklUkGCUmSJKlMMkhIkiRJZdJIkFAUpb2iKOcU\nRbmgKMq4Ut4frShKjKIoJxRF2a0oSh1NHFeSJEl6tsodJBRF0QbmAe8CdsCHiqLYldjtGOAuhHAC\nNgDfl/e4kiRJ0rOniZ5EY+CCEOKSECIHWAN0LrqDECJUCJH54GUUUEsDx5UkSZKeMU0EiZrAtSKv\n4x9sK8vHwB8aOK4kSZL0jOk8z4MpitIHcAdalPH+EGAIwFtvvfUcz0ySJEkqjSZ6EglA7SKvaz3Y\nVoyiKK2BCUAnIUR2aQ0JIX4WQrgLIdwtLS01cGqSVNygQYOIiYnReLsmJiYab1OSXgaa6EkcAhoo\nilIXVXDoBfQuuoOiKK7A/4D2QojbGjimJD2VX3755UWfgiS9UsrdkxBC5AHDgR3AGWCdEOK0oihT\nFEXp9GC3mYAJsF5RlGhFUbaU97iS9CgZGRl06NABZ2dnHBwcWLt2LT4+Phw+fBiAxYsXY21tTePG\njRk8eDDDhw8HoH///owYMYJ33nmHevXqsWHDBgDS09Np1aoVjRo1wtHRkc2bN7+wa5Ok50UjYxJC\niO3A9hLbJhX5ubUmjiNJT+LPP/+kRo0abNu2DYC0tDQWLFgAwPXr15k6dSpHjx7F1NQUX19fnJ2d\n1Z+9ceMGe/fu5ezZs3Tq1Inu3btjYGDAxo0bqVChAklJSXh5edGpUycURXkh1ydJz8NzHbiWpGft\n/IGb7N98kfTkbNJFDn+E/MkXX3xBx44d8fb2Vu938OBBWrRoQaVKlQDo0aMH58+fV7/fpUsXtLS0\nsLOz49atWwAIIRg/fjzh4eFoaWmRkJDArVu3qFat2vO9SEl6jmSQkF4b5w/cJDT4LHk5BQCYKFUZ\n/f488g2vMnHiRFq1avXYbenr66t/FkIAEBwcTGJiIkeOHEFXVxcrKyuysrI0exGS9JKRuZuk18b+\nzRfVAQIgNSMJrQI9quS4MnbsWI4ePap+z8PDg7///puUlBTy8vL47bffHtl+WloaVapUQVdXl9DQ\nUK5cufJMrkOSXiYySEivjfTk4jOrrydfZubGYUz4uR+TJ09m4sSJ6vdq1qxJ06ZNsbW1pWnTplhZ\nWWFmZvav7fv7+3P48GEcHR1ZsWIFDRs2VL+Xk5PDrl27AJg9ezaZmZllNSNJrxSlsCv9snF3dxeF\ns1Ak6XEsHx/5UKAAMKmkT7/pTR/anp6ejomJCXl5eXTt2pWBAwfStWtX8vLy0NF5+iexVlZWHD58\nGAsLi6duQ5KelqIoR4QQ7ppqT/YkpNdGk8710dEr/r+0jp4WTTrXB2DFihU4OTnh7OxM3759adu2\nLTVq1MDBwYFjx44RFhaGu7s7c+bM4datW3Tt2hVnZ2ecnZ3Zt28fcXFxODg4qNsODAwkICAAUE2b\n3bBhA0FBQVy/fp2WLVvSsmXL53btkvSsyIHrxxAREcHQoUPR1dXl119/5dixY/Tu3fvRH5SeK2tP\n1SyjwtlNJpX0adK5Ptae1Th9+jTTpk1j3759WFhYkJycTFBQECYmJowZMwYfHx9yc3PVayj8/Pxo\n0aIFGzduJD8/n/T0dFJSUh55DiNGjGDWrFmEhobKnoT0WpBB4jEEBwfz5Zdf0qdPH8LCwli9erVG\ng4QQAiEEWlqyY1de1p7V1MECVDOelo+PJCR8DTaWXiRfzMPCAvXU16L8/PzUP+/Zs4cVK1YAoK2t\njZmZ2WMFCUl63byx30qlrcbdvXs3rq6uODo6MnDgQLKzs/nll19Yt24dX331Ff7+/owbN46IiAhc\nXFz48ccf6dChAydOnADA1dWVKVOmADBp0iQWLVpU5irduLg4bGxs+Oijj3BwcODatWvs3LmTJk2a\n0KhRI3r06EF6evoLuz+vg8IpsYXjFDn38wkNPsv5AzdL3d/Y2Phf29PR0aGg4J/ZU3L6q/QmeGOD\nROFq3OPHj3Pq1Cnat29P//79Wbt2LSdPniQvL48FCxYwaNAgOnXqxMyZMwkODmbGjBl4e3sTHR3N\nqFGj8Pb2JiIigrS0NHR0dIiMjARUj6iaN2+uXqV79OhRQkND+e9//6uedx8bG8uwYcM4ffo0xsbG\nTJs2jV27dnH06FHc3d2ZNWvWi7xFr5Rly5ap02oUKjol1qaGK8cu/U3q3RT2b75IcnLyv7bXqlUr\n9ers/Px80tLSqFq1Krdv3+bOnTtkZ2cTEhJS6mdNTU25d++eBq5Kkl68NzZIODo68tdff/HFF18Q\nERFBXFwcdevWxdraGoB+/foRHh7+yHa8vb0JDw8nMjKSDh06kJ6eTmZmJpcvX8bGxka9StfJyYnW\nrVurV+kC1KlTBy8vLwCioqKIiYmhadOmuLi4sHz5cjkPv5yKznSqXsmKdq7+zNkymgn/+4jRo0f/\n62fnzJlDaGgojo6OuLm5ERMTg66uLpMmTaJx48a0adOm2BTYooYMGUL79u3lwLX0WnizxiROrIPd\nUyAtHmuzWhxdOYntV/SYOHEivr6+T9Wkh4cHhw8fpl69erRp04akpCQWLVqEm5sb8O+rdIs+3hBC\n0KZNG3799dfyX+crpkuXLly7do2srCxGjhzJkCFDMDExYeTIkYSEhGBoaMjmzZupWrUqW7duZdq0\naeTk5FC5cmWCg4OpWrVqsfbi4uIYOHAgsaeuYqRbgT4+Y6lkWpX61RzZG7OVPJFNrVq1CAwMVD/S\nCwsLK9ZG1apVS03gN2LECEaMGPHQ9mXLlql//vzzz/n888/Lf2Mk6SXw5vQkTqyDrSMg7RoguB5/\nBaPdX9LHSY+xY8eyf/9+4uLiuHDhAgArV66kRYuHayOVfJSgp6dH7dq1Wb9+PU2aNMHb25vAwECa\nN28OPP4qXS8vLyIjI9XHz8jIKJZL6FWVmprK/Pnz/3WfJUuWcOTIEQ4fPkxQUBB37twhIyMDLy8v\njh8/TvPmzVm0aBEAzZo1IyoqimPHjtGrVy++//7hcumff/45/fr1Y/fWvXjatmbDvp8AmL99HA3f\ncmX31kjOnTtH0TVC06dP1+BVS9Lr483pSeyeArn31S9P3ipg7KoktBb1Q7eGAwsWLCAtLY0ePXqQ\nl5eHh4cHQ4cOfagZJycntLW1cXZ2pn///upxid27d2NoaIi3tzfx8fHqZHL+/v68//77ODo64u7u\nXuYjCktLS5YtW8aHH35IdrbqMcm0adPUj79eVYVBYtiwYept5w/cZO/Gc9xPzcekkj6Hb28k/NBf\nAFy7do3Y2Fj09PTo2LEjAG5ubvz1l+r9+Ph4/Pz8uHHjBjk5OdStW/ehY+7fv5/ff/8dXV1dvpj8\nOV5tfgYgM+ceP8z+DmvPahw8eLDYZ6ZPn8748eOf6Nry8/PR1tZ+os9I0qvmzVlxHWAOlHatCgSk\nau44UjG9evVi8+bN2NjYoKuri8jVIveuDjeSr/B1rxXM2jSC+DsXqVWrNl98OYbVq1cTEBBAy5Yt\nGT9+PCEhIdy/fx9XV1fWrl2Lvb096enpmJubA1CxYkX69+9PUFAQiYmJaGtrk5iYSEBAAKtXr0ZP\nT4/Y2FiSk5PR19dnxYoV3Lp1izFjxpCXl0eLFi3w9PRk5syZODo6Ym9vT3BwMKtWrSIoKIicnBw8\nPT2ZP38+2tramJiY8Mknn7Br1y7mzZtHs2bNXvAdlqTi5Irrp2VW68m2SxoxY8YM6tevT3R0NDNn\nzuRkzAk+8BrG171UaxCa2b2PdQ0XxnVfyMyZM9m/f7/6s4WPm2xtbdWP3i5dusTChQs5fvw49vb2\n6n0TEhJ49913OXToEDk5OcTExHDs2DHMzc2pWbMmoOqtRUVFMWLECCpUqIChoSGhoaHMmDEDQ0ND\noqOjCQ4O5syZM6xdu5bIyEiio6PR1tYmODgYUD0G9PT05Pjx4zJASG+ENydItJoEuobFt+kaqrZL\nz00dSxssKlRXv76Veo3YG8cZNqs9cXFxxb74Cx831atXj4yMDABatGjBBx988FBN6YYNG6Knp4el\npSVVqlTh/PnzODk5cfXqVRo3bgxA48aNCQkJwcnJidzcXExNTR86PxMTE3bv3s2RI0fw8PDAxcWF\n3bt3c+nSJUC1sK5bt26avSmS9BJ7c8YknHqq/vtgdhNmtVQBonC7pFGFxX+uXLlC6q1M9QI2I8N/\nZnSdvx5N7I3jfNt3A5WqmrF03wQCAgLw8fHB2NhYXfGtadOm6tXOf/75JwcOHGDbtm2sWLGCI0eO\nsHXrVhwdHfnpJ9UAta6uLps3b8bCwoJly5apU20YGRkxffp0evTogaWlJS4uLqWeuxCCfv368e23\n3z70noGBgRyHkN4ob06QAFVAkEHhmSta/Edf15DMrAxCg89i1DCZitWM0NHTIi+ngKycDIz0TDAy\nNqKKcwFRs6JKbS8rK4tdu3bh7OzM/fv3mTp1KuHh4RgZGXHt2jXCw8P5/fffOXjwIM7OzuqFcv37\n9ychIYGTJ0+yfft2zMzMiIiIYPLkySQnJ3Px4kUcHR2ZNm0aurq65ObmAqqFdB07diQ0NJSsrCyy\nsrKYOnUqPXr0eG73UJJeFm/O4ybpuSm60tnEwIx61RyYvGoAX0+dgLG5Pi39G2JSSR/b2h4oOoLv\ntwzmf7/+oF5YWFJ0dDRGRkYcP34cBwcHpkyZwqFDh3BycsLS0pItW7bQvXt3IiMjOXv2bLHPpqam\n0q1bN0JCQrh48SI//PAD0dHRzJgxAy0tLczMzPjvf//L4MGDcXJyIisrCzs7O5o2bcqVK1coKCjA\n2Ni41FxPmiRrUEgvK43MblIUpT0wB9AGfhFCzCjxfnNgNuAE9BJCbHhUmy9LPYmwsDACAwPLTMEg\nPWze0D1lvvfZwidftHj+/Hnatm2Ln5+fula1j48PgYGBxMfHs3HjRpYvXw5AUFAQ58+f56effqJ/\n//60adMGf39/4J81Lrm5uYwaNUpdq/rcuXNcvnyZatWqYWJiQnp6OuHh4QwcOJA+ffrQpUuXMh9N\naYqsQSFpiqZnN5X7cZOiKNrAPKANEA8cUhRlixAipshuV4H+wJjyHk96+ZlU0i+z+M/jKhzTKEz5\nve7nPzh/+0ixWtXbtm1j6dKl3L17l759+zJ16lR++uknkpOTOXPmDBUrVkRfX5/+/ftjaGhIZmYm\n9erVo2fPnmzbtg19fX28vLxITU0lKysLExMTsrOzsbe3p1q1amzcuJGoqCj8/PzIz8/HyMiI+vXr\ns2TJEipWrIiPjw+enp6EhoaSmprK4sWL8fb2Jj8/n3HjxhEWFkZ2djafffYZn3zyCWFhYQQEBGBh\nYcGpU6dwc3Nj1apVzJ07V12DwsLCgl27dvHxxx9z+PBhFEVh4MCBjBo1SmO/H0l6Epp43NQYuCCE\nuCSEyAHWAJ2L7iCEiBNCnAAKSmvgWYmLi6Nhw4b4+/tja2tL9+7dyczMZMqUKXh4eODg4MCQIUPU\nK28vXLhA69atcXZ2plGjRly8eLFYe4cOHcLV1ZWLFy9y8OBBmjRpgqurK++88w7nzp0DIDMzk549\ne2JnZ0fXrl3x9PRUD5yWleUFKBCSAAAgAElEQVR13Lhx2NnZ4eTkxJgxr34cfVTxn0cpmb01/loC\nBzddo3GD1upa1RkZGSxZsoStW7dSoUIFJk+ezPDhw1EUhV69euHv719swVxKSgoGBgb8+OOPzJ49\nGy8vL2JiYti3b596FXxGRgba2tqcPn0aZ2dnFi5cyODBg0lPT8fFxYUTJ07g6OjI5MmT1e3m5eVx\n8OBBZs+erd6+ePFizMzMOHToEIcOHWLRokVcvnwZgGPHjjF79mxiYmK4dOkSkZGRjBgxgho1ahAa\nGkpoaCjR0dEkJCRw6tQpTp48yYABA57+lyFJ5aSJIFETuFbkdfyDbS+Fc+fOMWzYMM6cOUOFChWY\nP38+w4cP59ChQ5w6dYr79++rHyX5+/vz2Wefcfz4cfbt20f16v9M1dy3bx9Dhw5l8+bN1K9fn4YN\nGxIREcGxY8eYMmWKerXu/PnzqVixIjExMUydOpUjR44AkJSUVGqW1zt37rBx40ZOnz7NiRMnitVh\nflVZe1ZTjzuAqgfR0r9hsToP/6bomAaoalV/u3YorTt5q2tVp6am0qpVKxwdHRk/fjzt2rVjx44d\nNG7cGDMzM/r27cvt27fVbbz//vsoioKjoyM1atQgLi4OZ2dnCgoK1OsotLS01DOXqlevzpIlS3By\ncuLOnTvMnDkTeDjx4wcffACoVoXHxcUBqn8MrFixAhcXFzw9Pblz5w6xsbGAahpurVq10NLSwsXF\nRf2ZourVq8elS5f4/PPP+fPPP6lQocJj3TdJehZeqtlNiqIMAYYAvPXWWxpps3bt2jRtqqpv3KdP\nH4KCgqhbty7ff/89mZmZJCcnY29vj4+PDwkJCXTt2hVQTXUsdObMGYYMGcLOnTupUaMGoMrJ1K9f\nP2JjY1EURT0zZu/evYwcORIABwcHnJycgOJZXgFycnJo0qQJZmZmGBgY8PHHH9OxY0f12oBXXcni\nP0+i5KMqu9oe2NX2AFQB58Avd/Gs2xH9B7UdevfuzZAhQ9QV5wq/uE1MTOjevTshISHo6+uTnp5O\nXFwcRkZG6kV7/fv3p2PHjlhZWQGq3ytAt27dWL16NWFhYTg6Opaa/gNAX18VCLW1tcnLywNUU2jn\nzp1Lu3btiu0bFham3r/kZ4qqWLEix48fZ8eOHSxcuJB169axZMmSx7+BkqRBmuhJJAC1i7yu9WDb\nExNC/CyEcBdCuFtaWj7d2ZxYBz86qNJwLG6Lkne/2NuKojBs2DA2bNjAyZMnGTx48COLx1SvXh0D\nAwOOHTum3vbVV1/RsmVLTp06xdatWx/ZRmGW1+joaKKjo4mJiWHx4sXo6Ohw8OBB9ZdZ+/btn+66\nXyP/NnZRGECszB35ffPvHNgZQ0BAAA4ODmRlZZGbm0uXLl0IDg5W5896XAUFBWzYoJpTsXr1apo1\na4aZmRkVK1YkIiICKDvxY1Ht2rVjwYIF6n84nD9/Xr0YsCxFE0cmJSVRUFBAt27dmDZtGkePHn2i\n65AkTdJEkDgENFAUpa6iKHpAL2CLBtp9ciUyvXLvBldvJLF/5TTgn7/4ABYWFqSnp6u/FExNTalV\nqxabNm0CIDs7Wz0l0dzcnG3btvHll1+qU0qnpaWpH1MUTRNta2urXpEbExPDyZMngbKzvKanp5OW\nlsZ7773Hjz/+yPHjx5/d/XlFlDamUVL1Sla0delNN/+O/PXXX7i7u3P69Gny8vJwdnZm5cqVzJkz\n54mOa2xszMGDB3FwcGDPnj1MmqRajb98+XLGjh2Lk5MT0dHR6u1lGTRoEHZ2djRq1AgHBwc++eST\nUnsMRRWtQZGQkICPjw/Ozs706dOn1EV9kvS8aGoK7HuoprhqA0uEEN8oijIFOCyE2KIoigewEagI\nZAE3hRD2Zbf4lFNgf3R4ECBU4lILaL8qE/c6phzJrI6dnR0rV65k+vTp/Prrr1SrVg1ra2vq1KlD\nQEAAsbGxfPLJJyQlJaGrq8v69eu5evWqegrs1atXadWqFQUFBaxatYp+/fphbGxMhw4dWLVqFXFx\nccTExNCkSRNq1qxJw4YNuXTpEuvXr6dBgwbs2bOHL774oliWV1dXV7p160ZWVhZCCMaMGUO/fv2e\n7LpfQyVnN5U2W6rQ00yrLU3h9NdnYcWKFQQGBqIoCk5OTvTs2bPUuhgBAQFcvHiRS5cu8dZbb72R\n9UWk8tH0FNjXKwtsiUyvcakFdFydyalhpqVmeh03bpy6FkRhIDAxMUEIwbp168jOzqZr165MnjyZ\nSZMmsW7dOlxcXNi0aRM2NjZkZWVRo0YNAgMDGThwIACtW7fmjz/+ICYmhpiYGBo3bkydOnXQ09Nj\n1qxZtGzZkmXLlvH777+Tnp5Ofn4+f//9d3lu1Rth+fjIMqfV9pveVCPHeFZB4vTp03Tt2pV9+/ap\nx00URcHc3BxFUfjll184c+YMP/zwAwEBAWzdupW9e/diaGj46MYlqYSXbp3ES8WsVrGexHd7s7lz\nX6gzvQYEBBQLAqmpqeqaBNevX+fbb7/Fy8uLEydO4OHhwcaNG/Hz82PRokV8++235OXl4enpydtv\nv82tW7cYMGAAN2/epGnTptSuXZuff/6ZTZs2ERsbS4UKFcjOzsbIyIhdu3bRsWNH2rVrR4MGDejc\nuTNHjx7lxIkTz3wl7+uiSef66lQfhZ5kWu3jKCtAxMXF0bFjR06dOlVs+6BBgxg9ejR2dnalBpgz\nEaFErFnBH/sP8bapIYlnTmLh3ZJKlSpx8uTJMutidOrUSQYI6aXxegWJVpNUYxIPigsNcdMj5g7q\nTK/BwcGkp6djbGyMjo4OTk5O7NixAxMTEy5fvkxOTg43b94kMzOT3bt3U6NGDSwtLdm7dy9CCPT0\n9AgNDSU7O5uUlBSCgoKoU6cOlpaWODg40K5dO6ysrNQzlipXroyxsbF6gVXDhg3R0dEhJCSEtLQ0\nvv/+exwdHZkzZw73799n06ZN1K+vuS+910nhTKmij6CadK7/1DOoNOGXX34p870zEaHs/Pkn8nKy\nAUHO/Qx2/qxKQGjr3ZLPP/+c0aNH06lTJ/Uiu0JFy9pK0ov2euVucuoJ7weBWW1AwbWhFbcVS65b\nNOP48eOYmZlx8+ZNMm5fwSA1lrA/N1GQl0tiYiIRERFYWFjQunVrgoKCmDZtGpUqVWLZsmVUqFCB\natWqUa9ePUJCQrC0tKR27dpYWloyY8YM0tPTqVixIp6ennz22WekpKTg6+uLjo4OsbGxzJ49G21t\nbW7dusXZs2fV6R5WrlzJ+fPnOXjwIIMGDWLu3Lkv+g6+1Kw9q9FvelM+W+hLv+lNn2uAyMvLe2hR\npo+PDyUfiSYlJdGkSRPmz/iGvJxsQs9eZP/Fq4Sdu8TGQ9FErFlBcnJysYkPhSlFJOll9Hr1JErR\no00TNsyZwM0jIXQwucs5PQhorsON9AJupgvOpwiO3Epn//79VK5cGW1tbUaPHo2JiQn37t0jKiqK\ngoKyF4qbmJhQUFDA9evXiY+PJzo6Gh0dHXJycrC2tubevXt07NgRMzMzOnTooF7Rq6+vT/369Wnb\nti0Ajo6OhIaGPq/bIj2hc+fOsXjxYpo2bcrAgQNLrdt969YtOnXqxLRp0zixOIhzN26TlJ7BmHbN\nORQXz6ZjpzkZv5Z9GYKAgAB69OhBxYoV8fX1Va/IlqSXzesVJAqnwBbWsk67hp9eAoN/TScpo4BV\nHxiw4FAOS6JzaFVXdel6SgF5OfepVKk+OTk5bN++nYkTJxIUFERBQQELFy7ExMSEO3fuYGpqqq5Q\nBlCtWjWCg4Np1KgR+/btIyMjg4oVK6rf9/X1JTo6GhsbGwwNDZk8eTKLFy/mypUrWFtbo6WlpV5c\npaWl9a/TJPPy8tDReb1+Xa+S0hZlFpWbm0urVq2YN28eLVq04PKm1Zw/dprzN5P48a+9AJga6POe\nu4t6ynTnzsWy1wAUe+wkSS+D1+tbZ/eUfwLEA/aVC7iXXUDNCgrrTueRmAk6WoL5h3LIyYeKBqCr\nBffv3ychIYG33nqLoUOH8t1336GlpcXEiRN5++238fb2Vs9MKcz1Y2JiQnJyMmfPniU3N5fBgwcz\ndOhQFi9eTGhoKJGRkejq6jJmzBjmz5/P+PHjEUKoex56enqAas3Ed999x/79+3F1dSUgIIDOnTvL\nWVAvUOGg8707SWTr6j8YW/hHYUGkQjo6Ori5ubFjxw5atGiBd6+P+DV0L7629WlSv45qHz192g4Z\n/tyuQZI04fUak0iLL3XzyU9NCGxjQMj5PBpUUvB+S5vMXKhfSYsGlbWxrmLAmjVrCAsL4+7du3h6\neqKrq4ubmxugqp/s5eWlXpF79OhR3NzcGDVqFGlpaVy+fBkbGxs2b96Mn58fFhYWrFu3jqlTp1K5\ncmXOnz/PqVOnsLGxoUqVKsTExDB//nx1UsBvvvmGRo0a0axZM0JDQxk7dqx6he7Ro0fZsGGDDBDP\nUeGg872kRBCCjJRkbty6za8LVAPPRRdlFlIUhSVLlnD27Fm+++47bL1b4v/xYI5cu0l2Xj6mFpY4\nd/Wjss2/Lg+SpJfO69WTKDEFtqiIq3m0ra/Nnsv5rO1hxOgdWVQyVPgmIhub+rUYNGgQoFpdfebM\nGVavXk14eDj9+/ena9euTJkyhSZNmmBpaamuUDZ79myys7MxMDCgS5cumJiYMHjwYBwdHWnRooXq\nX5Te3ur9zc3NmTVrFgDNmzencuXKvP322+zcuZOsrCx0dHTw8fEhKyuLq1evAtCmTRs5TfY5i1iz\n4qGeg6WpMbN//JEpQfOws7Pj008/ZevWrcX20dbW5tdff6VTp06Ympoy7L//R4aOvmoW1Pl4TI6e\nZZWzG1WqVHmelyNJ5fJ6BYkSU2ABULRB5JfYUfWooECvAubmuUSfjXuoqU6dOjF+/HiSk5M5cuQI\nvr6+ZGRkYG5uTnR09FOfYsnHFIqiIITgt99+w8bGpth7Bw4ckNMhX4B7d5KKva5kbMQX7/qAovDf\nNf8EhsIULfDPGgt9fX127Nih3j5y5Eh1wkdJehW9Xo+bSkyBxbASaKlSPzevo0NYXD6HBptwz64P\nW5Pewqj1/1G3gS3r168HVEn4CnMnmZiY4OHhwciRI+nYsSPa2tpUqFCBunXrlrp/oUclhFu7di2g\nyhZrZmaGmZkZ7dq1Y+7cueq6FkUTCUrPn2nl0qvDlbVdkl5nr1eQAFWgGHVKlYZDzxjycwBoVF0b\nP3tdnBem8+64pXh4qFJPBwcHs3jxYpydnbG3t2fz5s3qpvz8/Fi1ahV+fn7qbf+2f6F/SwhnYGCA\nq6ureoAbVBllc3NzcXJywt7enq+++uqZ3Brp8Xj3+ggdveKZaHX09PHu9dELOiNJenFer9xNJZXI\n5fQPpdRcTs9aYV1md3eNpVWRnpGis5tMK1vg3esjbL1bvujTkqRHkrmbnkRZA9kPcjlJUllsvVvK\noPAIubm5xMfHP7KWivRsGBgYUKtWLXR1dZ/pcV7vIFHaQLauoTqX0/NWdKBTkl518fHxmJqaYmVl\n9dCEDOnZEkJw584d4uPjy6yaqCmv35hEUSUHss1qq1479XzRZyZJr7ysrCwqV64sA8QLoCgKlStX\nfi69uNe7JwGqgCCDgiQ9EzJAvDjP696/3j0JSXpFBQQEEBgY+MSfW7ZsGcOHvzmpP+Li4nBwcHgm\nbT/NvbSysiIpSbXOxsTE5Fmc1nMng4QkPYHU1FR1Btjr16/TvXv3F3xG5fOo2tuSJIOEJD2BokGi\nRo0abNiw4aF9nvaL95tvvsHa2ppmzZqp83pdvHiR9u3b4+bmhre3N2fPngVg69ateHp64urqSuvW\nrbl169ZD7SUmJtKtWzc8PDzw8PAgMjISUPVS+vbtS9OmTenbt+9TnevT2HQsgaYz9lB33DaaztjD\npmMJGmm3tFofU6ZMwcPDAwcHB4YMGaJeqBoUFISdnR1OTk706tULUCXYHDhwII0bN8bV1bXY2qdr\n167h4+NDgwYNmDx5snp7ly5dcHNzw97enp9//lkj1/HSEkK8lH/c3NyEJL1s/Pz8hIGBgXB2dhbd\nu3cX9vb2Qgghli5dKt5//33RsmVL0bx5cyGEEN9//71wd3cXjo6OYtKkSUIIIdLT08V7770nnJyc\nhJWVlZg8ebIQQojDhw8LBwcHkZGRIdLS0kT9+vXFzJkzha+vrzh//rwQQoioqCjRsmVLIYQQycnJ\noqCgQAghxKJFi8To0aPV5/HZZ58JIYT48MMPRUREhBBCiCtXroiGDRsKIYT4+uuvRfXq1cX06dPL\ndS9iYmIee9+NR+NFw4l/iDpfhKj/NJz4h9h4NL5c53D58mUBiL179wohhBgwYICYOXOmuHPnjnqf\nPn36iC1btgghhKhevbrIysoSQgiRkpIihBDiyy+/FCtXrlRva9CggUhPTxdLly4V1apVE0lJSSIz\nM1PY29uLQ4cOCSGEuv3C7UlJSUIIIerUqSMSExOFEEIYGxuX69oeR2m/A+Cw0OB3sUYGrhVFaQ/M\nAbSBX4QQM0q8rw+sANyAO4CfECJOE8eWpOdpxowZnDp1iujoaHXt67i4OCZOnAigrlu+c+dOFi9e\nzG+//YatrS16enq0atWKxMREatSowbZt2wgICEBLS9WZj4iIoGvXrhgZGQGq3GFZWVns27dPnSAS\nIDtblXgwPj6+zBrZhXbt2kVMTIz69d27d9U5pqytrZ/5/PqiZu44x/3c4jnU7ufmM3PHObq41ixX\n26XV+qhbty7ff/89mZmZJCcnY29vz/vvv4+TkxP+/v506dKFLl26ALBz5062bNmiHgMqmWCzcuXK\nAHzwwQfs3bsXd3d3goKC2LhxI6DqbcTGxqr3e92UO0goiqINzAPaAPHAIUVRtgghYors9jGQIoR4\nW1GUXsB3gN/DrUnSqyUnJ4dWrVqRkpKCjo4On3/+OQMGDGDAgAHcvn2brl27oqenR35+Pr1798bC\nwoLbt28zdOhQ1q9fj46Wwv9mz6JOJTP0DAzwCtlKZl4+qamp9OjRA3Nzc1xcXDAwMODYsWP4+vqS\nnp5OmzZt0NfXx8zMjN69e/Prr7/yn//8BxcXFwAWLVqkrqy4bt06AgMDsbCw4NNPP6V+/frqWiag\neqT12WefkZiYiJGREYsWLaJhw4YavU/XU+8/0fYnUVrSzGHDhnH48GFq165NQECAeqrotm3bCA8P\nZ+vWrXzzzTecPHnyXxNsltZ2WFgYu3btYv/+/RgZGakzN7+uNDEm0Ri4IIS4JITIAdYAJUtudQYK\nC/luAFopcu6c9Io4ExHKz58N4Ide77P6qzFkZ2YUez8uLg4jIyOqVq3Kb7/9xqBBg/jggw+wtLSk\nevXqtGzZEl1dXeLj4/n6669JTU3FxMQEfV0dRHYWFBRw7sZtwk+fo46uwk9TJnHr1i3mzZtHWloa\nBw8eJD4+nsjISPr168fUqVMpKCjg999/58SJE5w7dw5LS0u2bt1Kfr7qX+tLly6lZcuWTJgwgWnT\nprFnzx6WL1/OnDlzHrq+IUOGMHfuXI4cOUJgYCDDhg3T+D2sYW74RNufxNWrV9m/fz9QvNaHhYUF\n6enp6nGjgoICrl27RsuWLfnuu+9IS0sjPT39XxNs/vXXXyQnJ3P//n02bdpE06ZNSUtLo2LFihgZ\nGXH27FmioqLKfQ0vM00EiZpA0dwX8Q+2lbqPECIPSAMe6pspijJEUZTDiqIcTkxM1MCpSVL5lCxA\nlHfvLndu3+JMxD/1yGvXrk1SUhKOjo707NmTOnXqkJWVxd27d0lISGDv3r3o6Ohw6tQppk6dipGR\nEZMmTcJUWyE9K5uRrZvyf+/6UFAg2HHyDH4DPqZWrVrqeugpKSmcOHECJycnNm/ezK5du5g8eTI9\nevTAzc2NmjVroq2trS6Xm5KSQm5uLitWrCA0NJTU1FSaN2/OwoULH6pNkp6ern6k5eLiwieffMKN\nGzc0fh/HtrPBUFe72DZDXW3GtrMp4xOPz8bGhnnz5mFra0tKSgqffvopgwcPxsHBgXbt2qmTeebn\n59OnTx8cHR1xdXVlxIgRmJub/2uCzcaNG9OtWzecnJzo1q0b7u7utG/fnry8PGxtbRk3bhxeXl7l\nvoaXWbkT/CmK0h1oL4QY9OB1X8BTCDG8yD6nHuwT/+D1xQf7JJXWJmgowZ8kldPPnw1QBYgigqOO\ncSPtHlVMTbh5Lx1D0wrcy7zPhx9+SHp6OnXq1OH48eNER0dz/fp1tLW1yc7O5u2332bw4MFMmDAB\nGxsbzp05g5YClU1UNUNupN7lAzcH3q5qwZrTl7l165Z6bKNjx47q6bZubm6sWbOGBg0aFDuvAwcO\nMH36dBo2bEidOnUYNmwYc+fO5ebNm3zzzTfF9g0ICMDExIQhQ4ZgY2PzVIHhzJkz2NraPvb+m44l\nMHPHOa6n3qeGuSFj29mUezziTVfa70DTCf400ZNIAGoXeV3rwbZS91EURQcwQzWALUkvtZIFiAD8\nvVwZ0645H73TiEHN3MlITUEHwU8//aTeR1EUqlWrRu3atRk8eDA6Ojo0adKE6Oho7O3tOXnyJFZV\nLKhnWZnRbb0Z3dYb2xpV0NPWpp6VFWPGjMHe3p6JEyc+VLOkTZs2zJs3T/06JSUFAE9PT65du8bq\n1av58MMPAfD19WX9+vXcuaP665acnFysrcepkaIpXVxrEjnOl8szOhA5zlcGiFeEJoLEIaCBoih1\nFUXRA3oBW0rsswXo9+Dn7sAeUd4ujCQ9B49baOh2coq6BvbevXuL1cAOCAigoKCAo0eP8scff6iL\nUPX7qC8XbicRuCOcS4nJtLJ9m6jL8XwXsocNGzbw+++/M3bsWPUXfKGJEyeSkpKCg4MDzs7OhIb+\n8+irZ8+eNG3alIoVKwJgb2/PhAkTaNGiBc7OzowePfqhc3+cGinSm0sj9SQURXkPmI1qCuwSIcQ3\niqJMQTVfd4uiKAbASsAVSAZ6CSEu/Vub8nGT9DIoHJMoWfO6qOSMTBaFH6Rutarc1dbDzs6OlStX\n8t5776nrh5iYmJCenk52djadOnWic+fODBs2jPEjhrM8OJiC/HwMDQxYMHsWVLRk7NixaGlpoaur\ny4IFCx67BknHjh0ZNWoUrVq10tQtKNOTPm6SNO95PG56vYsOSZIGFC1ApCgKoqCg9B1L1MB+nlJT\nU2ncuDHOzs7qR0fPmgwSL97zCBKvfxZYSSqnogWIzkSEsv2nH0rd70XWwDY3N+f8+fMv7PjS60vm\nbpKkJ2Dr3RLnNu89tF3WwJZeVzJISNITaj1oGO8N/y+mFpagKJhaWNJ2yHBZ7vQlFxERgb29PS4u\nLpw5c4bVq1e/6FN6JcjHTZL0FGQN7FdPcHAwX375JX369CEsLIzVq1fTu3dvjbVfmBCvMB/X6+L1\nuhpJkl5eJ9bBjw4QYK7674l15W4yIyODDh064OzsjIODA2vXrmX37t24urri6OjIwIEDyc7O5pdf\nfmHdunV89dVX+Pv7M27cOCIiInBxceHHH3+kQ4cOnDhxAgBXV1emTJkCwKRJk1i0aBHp6em0atWK\nRo0a4ejoqJ4mHBcXh42NDR999BEODg5cu3aNnTt30qRJExo1akSPHj3USRVfVbInIUnSs3diHWwd\nAbkPEvqlXVO9hnKVF/7zzz/VWXUB0tLScHBwYPfu3VhbW/PRRx+xYMEC/vOf/7B37171yvWwsDAC\nAwMJCQkBVNl1IyIiqFOnDjo6OuraGxERESxcuBADAwM2btxIhQoVSEpKwsvLi06dOgEQGxvL8uXL\n8fLyIikpiWnTprFr1y6MjY357rvvmDVrFpMmTXrqa3zRZE9CkqRnb/eUfwJEodz7qu3l4OjoyF9/\n/cUXX3xBREQEcXFx1K1bF2trawD69etHeHj4I9vx9vYmPDycyMhIOnToQHp6OpmZmVy+fBkbGxuE\nEIwfPx4nJydat25NQkKCutBTnTp11PmboqKiiImJoWnTpri4uLB8+XKuXLlSrmt80WRPQpKkZy8t\n/sm2PyZra2uOHj3K9u3bmThxIr6+vk/VjoeHB4cPH6ZevXq0adOGpKQkFi1ahJubG6Aaz0hMTOTI\nkSPo6upiZWWlTg9ubGysbkcIQZs2bfj111/LdV0vE9mTkCTp2TOr9WTbH9P169cxMjKiT58+jB07\nlv379xMXF8eFCxcAWLlypToNSlGmpqbcu3dP/VpPT4/atWuzfv16mjRpgre3N4GBgTRv3hxQPcaq\nUqUKurq6hIaGltk78PLyIjIyUn38jIyMV379igwSkiQ9e60mgW6J2hG6hqrt5XDy5EkaN26Mi4sL\nkydPZtq0aSxdupQePXrg6OiIlpYWQ4cOfehzTk5OaGtr4+zszI8//gioHjlVqVIFQ0NDvL29iY+P\nx9vbGwB/f38OHz6Mo6MjK1asKLMok6WlJcuWLePDDz/EycmJJk2aqOuSv6pkWg5Jkp7KE6flOLFO\nNQaRFq/qQbSaVK5Ba0mm5ZAk6XXi1FMGhVeQfNwkSZIklUkGCUmSJKlMMkhIkiRJZZJBQpIkSSqT\nDBKSJElSmWSQkCTpjeHj40Ph1Pr33nuP1NTUF3xGLz85BVaSpDfS9u3bX/QpvBJkT0KSpOdi26Vt\n/9/encdVXeWPH38dAQVRUUBF3FBTQZbLJmVK4m5TLmWk5kamppZapqPlZIw6oyXl0rT5nWncN2zG\nJaefkyKppSYmuOAGiqPigiiIArK9f38AN0BQ9CKgnOfjcR/c+7nnfj7nHuC+7+dzznkfem7oiccy\nD3pu6MnWM1tN3mdcXBzOzs4MGTIEFxcXXnnlFVJTU4tNF16Uk5MT165dA2D58uV4eHhgMBgYNmwY\nAAkJCQwYMID27dvTvn17Y2bYqsakIKGUslVK/aiUOp33s14J5f6fUipJKfW9KcfTNO3xtPXMVoJ/\nCebS7UsIwqXblwj+JXapwcwAACAASURBVLhMAsXJkycZP348x48fp06dOnz22WcEBQWxbt06jhw5\nQlZWFl999VWJrz927Bhz5swhLCyMqKgoFi1aBMCkSZN49913OXDgAN999x2jRo0yua6PI1PPJKYD\nO0SkNbAj73Fx5gPDTDyWpmmPqUW/LSI9O73QtvTsdBb9tsjkfTdt2pSOHTsCMHToUHbs2PFA6cLD\nwsIIDAzE3t4eAFtbWwC2b9/O22+/jaenJ3379uXmzZuP/QJCD8PUPol+QEDe/WVAODCtaCER2aGU\nCii6XdO0quHy7csPtP1BKKUKPa5bty6JiYkm7zcnJ4d9+/ZhaWlp8r4eZ6aeSTQUkUt59y8DDU3Z\nmVJqjFIqQikVkZCQYGLVNE2rLBysHR5o+4P43//+x969ewFYvXo1vr6+pUoXnq9r166EhoYaA8v1\n69cB6NmzJ59//rmxXGRkpMl1fRzdN0gopbYrpY4Wc+tXsJzkppM1KaWsiCwREV8R8a1fv74pu9I0\nrRKZ5D0JS7PC38gtzSyZ5D3J5H23bduWL774AhcXF27cuMG7775bqnTh+VxdXZkxYwadO3fGYDAw\nefJkABYvXkxERAQeHh60a9eOr7/+2uS6Po5MShWulDoJBIjIJaVUIyBcRNqWUDYAmCIiL5Zm3zpV\nuKZVbg+aKnzrma0s+m0Rl29fxsHagUnek3ih5Qsm1SEuLo4XX3yRo0ePmrSfx9XjkCp8MzACmJf3\nc5PJNdI07Yn0QssXTA4KWvkztU9iHtBDKXUa6J73GKWUr1Lq7/mFlFK7gVCgm1LqglKql4nH1TRN\nw8nJqcqeRZQXk84kRCQR6FbM9ghgVIHH/qYcR9M0TasYesa1pmmaViIdJDRN07QS6SCh3eXrr79m\n+fLlFV0NTdMqAR0kKrHFixfj4uLCkCFDSlW+YBpkU4wdO5YzZ84QEhJi8r40TXt4SUlJfPnllxVa\nBx0kKrEvv/ySZs2aceLECVxdXVmyZAkA//jHP2jTpg1+fn6MHj2at99+G4CMjAz++Mc/3pW1Mjg4\nmJEjRxIQEEDLli1ZvHgxANnZ2cVmvwwODuaXX34BIDY2lt69e+Pj44O/vz8nTpwAIDQ0FDc3NwwG\nA88991y5toumVYSsrKxyP2ZlCBKISKW8+fj4SFX25ptvioWFhTg7O8ucOXNk2LBhYmVlJe3atZP6\n9etLYmKiJCcni729vdStW1f69+8vtra28n//938iIrJixQqxtLQULy8vcXFxET8/P0lPT5cmTZqI\npaWleHp6yieffCINGjQQg8EgHh4e8uKLL8rt27flo48+kh49esj8+fOla9eucurUKRER2bdvn3Tp\n0kVERNzc3OTChQsiInLjxo2KaSStQkVHRz9Q+aTNm+VUl64S7ewip7p0laTNm8ukHrNmzZI2bdpI\nx44dZdCgQTJ//nw5dOiQPP300+Lu7i79+/eX69evy/Hjx6V9+/bG1509e1bc3NxERCQiIkKee+45\n8fb2lp49e0p8fLyIiHTu3FkmTZokPj4+EhISIiNGjJAJEyZIhw4dpEWLFhIaGioiIjt37pTnnntO\n+vbtKy1atJBp06bJypUrpX379uLm5iYxMTEiInL16lV5+eWXxdfXV3x9fWXPnj0iIvLRRx/J66+/\nLp07d5YWLVrIokWLRERk4MCBYmlpKQaDQaZMmXLXey/udwBESBl+Fld4MCjpVtWDhIhI8+bNZerU\nqdKgQQNp2rSp1KlTR2bOnCm1a9eWW7duyaeffipPP/20vPXWWxIVFSWAtG7dWlxdXcXa2loaNWok\nKSkp0q1bN+natatYW1tL8+bNpX79+nL+/HlZvHixvPvuu8bjzZgxQxYvXmwMEnPmzDH+gTZs2FBa\ntmwpzs7OsmDBAnnjjTeke/fusmTJEunWrVuZBIpDhw7J1q1bTd6PVj4eJEgkbd4sxw2eEt3W2Xg7\nbvA0OVD8+uuvYjAYJC0tTW7evClPPfWUzJ8/X9zd3SU8PFxERD788EOZNGmSiIgYDAY5c+aMiIjM\nmzdPZs+eLRkZGdKhQwe5evWqiIisXbtWXn/9dRHJDRLjxo0zHm/EiBHyyiuvSHZ2thw7dkxatWol\nIrlBwsbGRuLj4yU9PV0cHR1l5syZIiKycOFC4/EHDx4su3fvFhGRc+fOibOzs4jkBokOHTpIenq6\nJCQkiK2trWRkZMjZs2fF1dW1xPdfHkFCr0xXySRv2cKlv/wVSUri1vnzbPvySxwaNiTbyorr16+z\nfPlysrOz+d///seuXbvw9c2dfe/h4YGZmRn//Oc/uXHjBkFBQTRo0IBOnTpx8eJFWrRoYTxGvXr1\njKfOCQkJ+Pv7k5SUxK1bt+jVqxcODrlJ10SEunXr3pXYzMnJiYiICGJjY9m6dSsxMTFkZ2cDUKtW\nrXumU05KSmL16tWMHz8egPj4eCZOnMiGDRuIjIwkIiKCP/zhD2XXoFqlcHXBQiS9cKpwSU/n6oKF\n2PTp89D7/fnnn+nXrx+WlpZYWlrSp08fbt++TVJSkjGp34gRIwgMDATg1VdfZd26dUyfPp1169ax\nbt06Tp48ydGjR+nRoweQexm2UaNGxmMMHDiw0DH79+9PtWrVaNeuHVeuXDFub9++vfF1rVq1omfP\nngC4u7uzc+dOIDf9eHR0tPE1BdOPv/DCC9SoUYMaNWrQoEGDQvuuSLpPohJJ3rKF+Pc/QPLW3c0B\naouQFR9P8IABZGVlMWfOHOrXr4+DgwM5OTns3r3b+Po6deqwfv16RIQePXqwdOlSIiMjeeutt3j1\n1VeN5apVq8Y333zDN998w8qVK2nRogVHjhzhvffeY//+/Xz++ef8+uuvfPfdd9SoUYPQ0FCCgoII\nDQ1l2rRpxMfH8+yzzzJ9+nRmzZrFpUuXOHz4MHFxcaSmphIUFESbNm0YMmQI27dvp2PHjrRu3Zpf\nf/2VpKQkQkJC6NChA15eXrzyyiv85S9/ISMjg5kzZ7Ju3To8PT1Zt24dt2/fZuTIkfj5+eHl5cWm\nTTrry+Mq69KlB9r+qAwcOJD169dz6tQplFK0bt0aEcHV1ZXIyEgiIyM5cuQI//3vf42vsba2LrSP\nGjVqGO9Lgdx3BbdXq1bN+LhatWrGL2X56cfzj3Xx4kVq1ap11+vNzMwqpA+kODpIVCJXFyyEAn8Y\nlkqRLRCfkcm7n3zCM888Q+PGjRk6dCh+fn5ERUWRmpqKjY0NR48eJTk5mejoaKZNm8aGDRv4+OOP\ngdwO7YKp15OTk/niiy/w8vJCKcX69euxtLTknXfe4dixY7zxxht4e3tz5swZUlJSGDVqFGvXrmX4\n8OFYW1tTs2ZNzMzMuHbtGm5ubmRlZVGnTh0g95/mvffeIyIigi1btjBw4ECSk5MJDAzkr3/9K9On\nTyc+Pp7U1FS6d+/O+PHjad++PdWrV+fDDz/E3t6e7Oxs5s2bx5gxY+jatSvjx4+ncePGvPbaa7Rq\n1Yo//vGP5fuL0UxmXuCbeWm2l1bHjh3ZsmUL6enp3Lp1i++//x5ra2vq1atn/AJVMFV4q1atMDMz\nY/bs2cYzhLZt25KQkGBMN56ZmcmxY8dMqldJHjT9eO3atUlJSXkkdSktHSQqkaLfqpRSfOroSFir\nVnSwtCQxMZG33nqLiIgITp8+zdGjR0lNTWXZsmXMnDkTX19f5s6dS3R0NNu2bSMmJgZXJyf+tXgx\nzVatRtLTyUlLo1u3bqSkpLB//34cHBy4c+cODg4OdOjQATs7O2rVqsWzzz5Leno6b775JsnJyTRq\n1Ih27doxc+ZMAAYNGsSRI0c4evQoZmZmrFmzhrS0NJRSuLu7U7NmTV544QUWL15MeHg4q1atIi4u\njnnz5tG8eXNatmzJDz/8wJw5c4zrD+/YsQOAI0eOsGbNGkJDQ5k7dy4zZ87kxx9/xNbWlu+++451\n69Zx/vz58v3laCZp8O47qCKL9yhLSxq8+45J+23fvj19+/bFw8OD559/Hnd3d2xsbFi2bBlTp07F\nw8ODyMhI498t5J5NrFy50nh2Xb16dTZs2MC0adMwGAx4enoaR/eVtQdNP25nZ0fHjh1xc3Nj6tSp\nj6RO91WWHRxleauKHdenunQt1LFX8HaqS1djuffee08MBoO0bdtWJkyYIDk5OcXur2hnoZVSctzg\nKcN69BA7OzsREdmyZYvY2dmJwWAQR0dHsbe3l5EjR8qhQ4ekcePGMn/+fBER6dq1q3h7e4uIiI2N\njQQHBxuPY2FhIQ4ODnLw4EFRSomISEZGhjg7O0uzZs3EYDBI9erVpW3btnL27FmpW7eucfTG7t27\nxcLCQkREvLy8pF+/fsb9Wltby6ZNm+Sf//ynjBo1yri9d+/exs4/reJUltFNKSkpIiJy+/Zt8fHx\nkYMHD5bJfh8HuuO6imnw7jvEv/9BoUtOAMrCotA3rtJOciups7DdsWOEpqRw69YtRIT69euTkpKC\nvb0958+f56mnniIqKor4+Hg+/fRTfv75ZzIzM0lPT8fPz48aNWqQlpZGXFwcffr0ITs7m4SEBAYP\nHmy8Rrtq1SrS09P5+OOPGTRoEE2aNDE+l52dTePGjQHYsGGDsW7m5uakpqYaH9erV4/Vq1fTq1cv\natSowaFDh/Dy8qpU12u10rPp08ekTuqSjBkzhujoaNLT0xkxYgTe3t5lfoyqTAeJSiT/Hyh/dBOA\nWd26NJzxwUP9c5XUKehraUmtnByednMjVSnOnTvH1q1b6dSpE82aNWP+/PncuXMHCwsLJk6cSFpa\nGosWLeKpp54iIyODjh078uWXXxIaGkqvXr04evQoDg4OrFmzxjjaKjk5GUtLS8zNzdm5cycXL16k\nTZs21K5dm5o1a/L+++8zZ84c4wL2AH379mXRokV4enoSFBQE5I6W+vDDD0lNTSUuLo7vv//+gdtB\ne7KtXr26oqvwRNNBopIpy29b5o0akRUfb3x8sM3viwbWrVaNjS1bsbVvH3777TdmzpzJnTt3MDMz\no0ePHuzevRsrKyu++eYbatWqhYWFBc7Ozri6upKdnc2tW7fYvXs3V69exdnZmZSUFJo1a0bNmjUB\nGDJkCGvXruXPf/4zvr6+ODs788MPP2BnZ0fXrl05fPgw3bt356233iI8PByAKVOmEBsbS0REBMuW\nLWP58uV06dKFpUuXEhERwd/+9rcyaRdN00rPpOVLHyW9fKnpkrds4dKHM++65FTQyuxssnx9+HjN\nGgCcnZ1JSkoiMTGRGTNmEBwcTExMDB06dOCDDz6gb9++BAYGsnbtWgYPHszBgweB3+dO2Nvbl8t7\n0yregy5fqpW98li+VI9ueoLZ9OlDo9mzwMysxDKGjAy2bNrEle++49atW+Tk5DBlyhTatWtHYmIi\nnp6exlFPb775ZrFDCKFyDNXTNK3s6SDxhLPp0wfHeXPvGn6Yz93Kii41a/LMsGF3DSE8cOAAOTk5\ndOzYkb179xovJRUdQgi5nYe9e/emS5cuJtc5Li4ONzc3k/YRHh7Oiy++aHJdNK2q00GiCsg/ozB3\ndCz2+ddt7fhPcye2bdvGuXPn8PHxwdPTk3379nH48GE2btxIvXr1jOWnTJmCiODk5GTcNmHCBE6e\nPGlMP/AolEXwKMnSpUuN2XRLy8nJiWvXrgEYZ81qFa8s10OZOXMm27dvB2DhwoWFRt9VFSYFCaWU\nrVLqR6XU6byf9Yop46mU2quUOqaUOqyUGljcvrRHy6ZPH1qH7Sg2UARfvszLF87j7e3NgAEDKsUQ\nwuzsbEaPHo2rqys9e/YkLS2N6Ohozpw5g4eHBy+99BI3btwAICYmhu7du2MwGPD29iY2NrbQvg4c\nOICXlxexsbElpvqYO3duoVnpnTp1IioqqvzesFZmxo4dy/Dhw+/a/jDDpmfNmkX37t0BHSQe1nRg\nh4i0BnbkPS4qFRguIq5Ab2ChUqquicfVHlJxM19DWrZk3+rVnDhxgvfff7+CalbY6dOneeuttzh2\n7Bh169blu+++Y/Lkydjb2+Pu7s7u3bvx8/MjNTWVgIAA4uLiyM7OxtPT05ig8OzZszg5OfHcc8/R\npEkTWrVqRXBwsDGIZGVlMX78eG7fvo2/vz8REREEBATQvHlzzpw5g8FgAHITuvn4+ODg4ECjRo1K\nvQhUvvDw8EIzeIOCggrND7mfgmdQj/NltFP7L7Psg5/5YmwYyz74mVP7L5fJfouuiRIcHGycSxQQ\nEMA777yDr68vixYt4sqVK7z00ksYDAYMBgO//PLLXWeoISEhBAcHA7//rhYvXkx8fDxdunQpk0uq\njxNTg0Q/YFne/WVA/6IFROSUiJzOux8PXAXqm3hc7SEVuvSkFOaOjjSaPeuRTHIyRYsWLfD09ATA\nx8eH2NhYUlJSOH/+POPHj2f//v1cv36dzz77jJycHGJiYjh69CiZmZmEhYUBcPLkSaytrYmNjWXF\nihUArFy5ktjYWDIyMgC4evUqJ06coH379pw7d461a9cSGBiIiBhX+fv22285ePAgNjY21KlTx7ho\nU2kVDRJV0an9l9m56gS3ruemYLl1/Q47V50wOVAcO3aMOXPmEBYWRlRUFIsWLbqrTEZGBhEREbz3\n3ntMnDiRzp07ExUVxW+//Yarq2upjjNx4kQcHR3ZuXPnI72kWhmZGiQaikj+jK3LQMN7FVZK+QHV\ngdh7ldMerfxLTy7Ho2kdtqPSBIiNhy7ScV4YnT4O42JKFhsPXWTjoYt89dNZPvv+EAkpd7B3cDRO\nwLO1tWXv3r3cuXOHp59+Gnd3d8LCwozJ2WrXrk18fDyfffYZ5ua5U4Ju3ryJlZUVkJtps1GjRtSs\nWZMaNWrQqlUr9uzZw7/+9S+GDh3Knj17gNx8O3Z2dpw+fZqYmBj+/Oc/079/f+Lj4+nduzeHDx8G\n4Pr16/Tv3x8PDw+eeeYZY2bcr7/+mgULFuDp6WlMOrd9+3Z8fX1p06aNcYJgXFwc/v7+eHt74+3t\n/UQFlr2bYsnKyCm0LSsjh72bTPsoCAsLIzAw0Dj02tbW9q4yBUfhhYWFMW7cOCD3929jY2PS8auC\n+06mU0ptBxyKeWpGwQciIkqpEiddKKUaASuAESKSU0KZMcAYgGbNmt2vatpjquiaEpAbIN7/1xHS\nMnPXpcjKzmHqhigQSE7LRNWoiVS34sbNW2w8dJHITSswGAxkZ2dz8+ZNRo8ezahRo/jTn/5kHIr7\n7LPPMnLkSMaOHcv69es5c+YMtWvXxs/Pj5UrV6KU4tChQ7i4uLB//35at27NxIkT8ff3x8rKip9+\n+glnZ2e2b9/O+fPnadeuHY0bN+bChQt4eXkRGRnJjBkzeP755xERPvroI7y8vNi4cSNhYWEMHz6c\nyMhIxo4dS61atZgyZQqQu/xsXFwcv/76K7GxsXTp0oWYmBgaNGjAjz/+iKWlJadPn2bw4MFlsmZ5\nZZB/BlHa7WWpaKrvoszNzcnJ+f0jKf0e84qqovueSYhIdxFxK+a2CbiS9+GfHwSuFrcPpVQdYCsw\nQ0T23eNYS0TEV0R869fXV6TKQ2VZt3f+tpPGAJEvM1vIzPn9e0e9Lm+QnZrE4O5+REZGYmVlRadO\nnahTpw4rV67Ezc2NTz/91DjfIz09nQEDBrBv3z4uXbpEeHg4w4YNIzIyEg8PD1xdXZk4caJx/7/9\n9hvW1ta89tprbNy4kf3795OQkEC9evWoWbMmmZmZREREcOTIEeN64P7+/ly5cgURYc+ePcbtXbt2\nJTExkZs3bxbbBq+++irVqlWjdevWtGzZkhMnTpCZmcno0aNxd3cnMDCw0OI0j7tatjUeaHtpde3a\nldDQUBITE4Hcs7l76datG1999RWQOzgiOTmZhg0bcvXqVRITE7lz506JqV+q6lwgUy83bQZG5N0f\nAdy1KoxSqjrwb2C5iJS+t66Kmz17Nm3btqVTp04MHjyYkJAQIiMjeeaZZwqN7jlx4gR+fn7G18XF\nxeHu7g7AwYMH6dy5Mz4+PvTq1YtLebmcinbmBQUFMXHiRJ599llatmxp7FQNDw+nc+fO9OvXj5Yt\nWzJ9+nRWrVqFn58f7u7uxg7ghIQEBgwYQPv27Wnfvj0///wzAMHBwYwcOZKAgABatmxpvJY/ffp0\nYmNj8fT0NKY/jk9KM74Hc5uGOL7xexCxefpl6nYaQnX7ZpjbNsHMoQ0nT54kNTWVcePGMW7cOM6f\nP4+NjQ2DBg2iXr16+Pv7c+fOHdzd3enXrx+zZ8+me/fuzJo1i06dOv2+77zLDXfu3CEzM5PY2FgC\nAwOxs7MjIeEab06ZyX+2h1OzQXMuX7mChYUF586dY+HChQAsWbKE7Oxs0tLSjCv0BQUF4ebmxuXL\nl/niiy+K/f0qpe56vGDBAho2bEhUVBQRERHGfpMnQYd+rTCvXvjjxrx6NTr0a2XSfl1dXZkxYwad\nO3fGYDAwefLke5ZftGgRO3fuxN3dHR8fH6Kjo7GwsGDmzJn4+fnRo0cPnJ2di31tWc4FeqyYkkIW\nsCN3VNNpYDtgm7fdF/h73v2hQCYQWeDmeb99V8VU4fmq4rq9z87dIc2nfV+q27Nzd5R5m7/99tti\nbW0t69evFxGRlT9Fi7lNA2kyYZXxuOZ16suyHYfl7bffFicnJ4mKipKdO3eKhYWFJCQkyIQJE2Ts\n2LHSvXt32blzp3h6esqNGzckJCTE2G757f38889Ldna2xMTESOPGjSUtLU3eeecdCQkJERGRb7/9\nVnL/PaVQe+3cuVNeeOGFMn//D+NBU4Wf3HdJlr6/R/725g5Z+v4eObnv0iOqWdVR6VOFi0gi0K2Y\n7RHAqLz7K4GVphynqqmK6/ZO7dW2UJ8EgIWZAqHQJScrCzOm9mpb3C4eysZDF5m/7STn7riTZVGL\nNVt34uDgwJe/ZFI0rVlORjrjXutHc1tL4uPjefHFF3F0dDR2mgYHBzNs2DB27drFkSNH+Oijj6hT\npw59+vThlVdeYdOmTcZVyZo1a4afnx83b97k66+/xtLSkvHjxzNgwACWL19O796973st/XHT5mkH\n2jxdXPemVpnpLLCVSPKWLVxdsJArx46RUtOKZF/fUo08GjhwIIGBgbz88svGdXuPHDmCq6urcUnG\noh7Fur2WxaT+KO26vf29cteXmL/tJPFJaTjWtTIGg6Lb8suaqmBnubltY+oPX8i+c78x9p2pxNd8\nqlDZzKTLVLOshd2guUQvHEhQUBABAQEEBQUZZ57b2tqydetWbt26xbZt21ixYgUHDhzg22+/NY6A\ngtx+jOK0bt26ULn85WednJw4evQokHupMCAgoEzev6aVhg4SlUTBjK1elpYEX7pE3Iw/YZ+Wxvff\nf8+YMWOM6/b6+/s/0Lq9HTp0IDMzk1OnTpV6XPiDyF+3N79/ITIy0jjHoTgldQD292pcbAAoq6BQ\nVMHO8qyURMysamPm3JkcGxuqRfyHatWtyMlIw6ymDZKRirKoQZMGdly5coUffvjB+GGd/37s7e25\ndu0a1atXZ8CAAbRt25ahQ4c+krprWnnRuZsqiYKryLlbWdGlVi36nThOvzFj9Lq9DyAgIKDYYaPF\nzVQu2FmemRDH+cWDubBkDGf+u4z3P5hBPZ/nuRr6EZfXvE/1Bi2xavQUcd+M4bXXXiu0WFLBDs2L\nFy8SEBCAp6cnQ4cO5Q9/+AMuLi7Gzs7Bgwfj4eHBggULyvR9a9qjoteTqCSOu7Sj4EXw2zk5WFer\nRpoIY2rXYsmSJZUip1JlFxAQQEhIiHGFvHzh4eGEhIQUGt7YcV4YFwsEisurp1Ovyxu0dPHg5+ld\njf0Vplzq6t27N3/605/o1KkTly9fplOnTsTExJj2JisJvZ5ExdPrSVQh5gU6lSE36d5LcWd55cL5\nSpN0rzKJi4vDxcXlriSAAKGhofj5+dGmTRvjDOeir3V2dsZ81+dc+vs4Ev79V3Iyc8/iaphXY2qv\ntowbN445o/uRtGICw2v8ys/Tu1Lnxkn69/8988yPP/7ISy+9BMCaNWtwd3fHzc2NadOmAbnJ4fbs\n2cMbb7zB1KlT6dmzJxcvXiw081rTKr2yHCpVlreqNgQ2afNmOW7wlOi2zsbbcYOnJG3eXNFVq5TO\nnj0rZmZmcujQIRERCQwMlBUrVkjnzp1l8uTJIiKydetW6datm4gUHjp69uxZAWTPnj3y798uSAOf\n3lIvYKTUaWGQ+Su3iohIYmKiiIhkZWVJ586dJSoqSnJycqRt27bGYcWDBw+WzZs3y8WLF6Vp06Zy\n9epVyczMlC5dusi///1vEckdbnzgwAHjcYsO/X2cPegQWK3slccQWH0mUUk8Lon3KlJ+bqcW07cy\n4KtfaODYtFASwLi4OABefvnlu7YV1bRpUzp27Eh/r8as+WQqz9kk4tWsLgFtGwCwfv16vL298fLy\n4tixY0RHR6OUYtiwYaxcuZKkpCT27t3L888/z4EDBwgICKB+/fqYm5szZMgQdu3a9cjbQ3t4FZFp\n4HGlRzdVIjZ9+uigUIKiuZ2u3EwnMV3YeOgi/b0aY2ZmZrzclD/stuCQ2z2nE/gl5hotpm/FVpJJ\nzyycPqzgDOizZ88SEhLCgQMHqFevHkFBQcZ8Pq+//jp9+vTB0tKSwMBAY+JA7f6O797J7rXLSUm8\nRm07e/wHDcfF3/TZy8uXLyckJASlFB4eHrz66qvMmTOHjIwM7OzsWLVqFQ0bNjSmiT9z5gzNmjVj\nTd667tq96TMJ7bFQXG4nEWH+tpP3fe3GQxdZsuss6Vk5CLkBJuHyReYt3QzA6tWrC6XquHnzJtbW\n1tjY2BiHu+ZzdHTE0dGROXPm8PrrrwPg5+fHTz/9xLVr18jOzmbNmjXG4claruO7d/LfJX8j5VoC\niJByLYH/Lvkbx3eblna7uFThnTp1Yt++fRw6dIhBgwbxySefGMtHR0ezfft2HSAegA4S2mOh4HDV\n0mwvaP62k2RkS7jPwwAABtlJREFUFw4w5rZN+HTRYlxcXLhx44YxfTSAwWDAy8sLZ2fnu4a7AgwZ\nMoSmTZsaR5U0atSIefPm0aVLFwwGAz4+PvTr1+9B3+ITbffa5WRlFM74mpVxh91rTVtmtLhU4Rcu\nXKBXr164u7szf/58Y+p4gL59+xpTxWulo8+VtceCY12rQsNV85MAOtbN/YfPT8NdkL29PXFxcbSY\nvhXLZh5YNvMwPqeqVaNWr3c5Pu8F47bw8HDj/aVLl5ZYlz179jB69OhC2wYPHszgwYPvKltwnwVn\nTlc1KYnXHmi7KSZMmMDkyZPp27cv4eHhxlXm4P5pw7W76TMJ7bEwtVdbrCzMCm0rbR6n/EBS2u33\n4uPjw+HDh/VM6gdU287+gbaXVnGpwpOTk2ncOHc+y7Jly+71cq0UdJDQHgv9vRoz92V3Gte1QgGN\n61ox92X3Uk1uKxpgzG0a0mrsNw+VKPDgwYPs2rWrUE4q7f78Bw3HvHrhNjOvXgP/QcNN2m9xqcKD\ng4MJDAzEx8fHeBlKe3h6xrVWJZTF7GmtsAedcf2oRjdVZeUx41r3SWhVQknJA7Xy4+LfRQeFx5C+\n3KRpmqaVSAcJTdM0rUQ6SGia9tAqa59mVVBeba+DhKZpD8XS0pLExEQdKCqAiJCYmFjsapBlTXdc\na5r2UJo0acKFCxdISEio6KpUSZaWljRp0uSRH8ekIKGUsgXWAU5AHPCqiNwoUqY58G9yz1osgM9F\n5N5Ll2maVulZWFjQokWLiq6G9oiZerlpOrBDRFoDO/IeF3UJ6CAinsDTwHSllKOJx9U0TdPKgalB\noh+QP+99GdC/aAERyRCR/MxeNcrgmJqmaVo5MfUDu6GIXMq7fxloWFwhpVRTpdRh4DzwsYjEm3hc\nTdM0rRzct09CKbUdcCjmqRkFH4iIKKWKHeYgIucBj7zLTBuVUhtE5EoxxxoDjMl7eEspdf/FAsqW\nPVD2aSmfDLptSqbbpmS6be7tUbRP87LcmUm5m/I+xANE5JJSqhEQLiL3zJqmlPoW+I+IbHjoAz8i\nSqmIssx58iTRbVMy3TYl021zb49D+5h6uWkzMCLv/ghgU9ECSqkmSimrvPv1gE5AeZ8haJqmaQ/B\n1CAxD+ihlDoNdM97jFLKVyn197wyLsB+pVQU8BMQIiJHTDyupmmaVg5MmichIolAt2K2RwCj8u7/\nCHgULVNJLanoClRium1KptumZLpt7q3St0+lXU9C0zRNq3h6zoKmaZpWoiodJJRStkqpH5VSp/N+\n1rtH2TpKqQtKqb+VZx0rSmnaRinlqZTaq5Q6ppQ6rJQaWBF1LS9Kqd5KqZNKqRil1F3ZBZRSNZRS\n6/Ke36+Ucir/WlaMUrTNZKVUdN7fyY68dD1Vwv3apkC5AUopUUpVqtFOVTpIULq0IvlmA7vKpVaV\nQ2naJhUYLiKuQG9goVKqbjnWsdwopcyAL4DngXbAYKVUuyLF3gBuiMhTwALg4/KtZcUoZdscAnxF\nxAPYAHxSvrWsGKVsG5RStYFJwP7yreH9VfUgcd+0IgBKKR9yZ5P/t5zqVRmUJuXKKRE5nXc/HrgK\n1C+3GpYvPyBGRM6ISAawltw2Kqhgm20AuimlVDnWsaLct21EZKeIpOY93Ac8+vSllUNp/m4g90vo\nx0B6eVauNKp6kLhvWhGlVDXgU2BKeVasEihVypV8Sik/oDoQ+6grVkEak5tWJt+FvG3FlhGRLCAZ\nsCuX2lWs0rRNQW8APzzSGlUe920bpZQ30FREtpZnxUrriV9PogzSiownd4b4hSftS2FZpFzJ208j\nYAUwQkRyyraW2pNEKTUU8AU6V3RdKoO8L6GfAUEVXJUSPfFBQkS6l/ScUuqKUqpRgbQiV4sp1gHw\nV0qNB2oB1ZVSt0TkXv0Xj4UyaBuUUnWArcAMEdn3iKpaGVwEmhZ43CRvW3FlLiilzAEbILF8qleh\nStM2KKW6k/sFpHOBzNBPuvu1TW3ADQjP+xLqAGxWSvXNm29W4ar65ab7phURkSEi0kxEnMi95LT8\nSQgQpVCalCvVyV1QanllzMVVxg4ArZVSLfLe9yBy26iggm32ChAmVWMi0n3bRinlBXwD9BWRYr9w\nPKHu2TYikiwi9iLilPcZs4/cNqoUAQJ0kChNWpGqqjRt8yrwHBCklIrMu3lWTHUfrbw+hreBbcBx\nYL2IHFNKzVJK9c0r9g/ATikVA0zm3qPlnhilbJv55J6Jh+b9nRQNsE+kUrZNpaZnXGuapmklqupn\nEpqmado96CChaZqmlUgHCU3TNK1EOkhomqZpJdJBQtM0TSuRDhKapmlaiXSQ0DRN00qkg4SmaZpW\nov8PyAWaM+QmXBQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst = ['baseball', 'software', 'police', 'government', 'circuit', 'car']\n",
    "\n",
    "tsne = TSNE(perplexity=5, n_components=2, random_state=23, init='pca', learning_rate=1)\n",
    "\n",
    "for word in lst:\n",
    "    \n",
    "    arr = [model[word]]\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word, topn=5)\n",
    "    print(\"Words similar to\", word, \":\", [x[0] for x in close_words])\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        arr.append(model[wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    \n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords, label=word)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EUjkpEmfDQG"
   },
   "source": [
    "*    The dataset consists of documents. Each document is a datapoint. Formulate a methodology to represent each document as a vector using the word vectors. Mention the method employed to create the vector representation of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Mhn2BWqXpC6"
   },
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(words, min_count=10, size=100, window=5)        # CBOW model for the documents\n",
    "sg_model = Word2Vec(words, min_count=10, size=100, window=5, sg=1)    # Skip-gram model for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zuh0gX5zXgY7"
   },
   "outputs": [],
   "source": [
    "y_train = newsgroups.target\n",
    "x_train_cbow = []\n",
    "x_train_sg = []\n",
    "\n",
    "for t in words:\n",
    "    cbow = [0] * 100\n",
    "    sg = [0] * 100\n",
    "    for wd in t:\n",
    "        if wd in cbow_model.wv:\n",
    "            cbow = [sum(x) for x in zip(cbow, cbow_model.wv[wd])]\n",
    "            sg = [sum(x) for x in zip(sg, sg_model.wv[wd])]\n",
    "    x_train_cbow.append(list(normalize(np.array(cbow)[:,np.newaxis], axis=0).ravel()))\n",
    "    x_train_sg.append(list(normalize(np.array(sg)[:,np.newaxis], axis=0).ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "dIGK3q6WriUv",
    "outputId": "45ac65a2-f15b-4d91-e923-3852fed30b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 100)\n",
      "(18846, 100)\n",
      "(18846,)\n",
      "(18846,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train_cbow))\n",
    "print(np.shape(x_train_sg))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEC9UchJneg1"
   },
   "source": [
    "*    Split the dataset into training (70%), validation(10%) and testing(20%) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTs111ydlilB"
   },
   "outputs": [],
   "source": [
    "val = int(0.7 * len(y_train))\n",
    "test = int(0.8 * len(y_train))\n",
    "y_train, y_val, y_test = y_train[:val], y_train[val:test], y_train[test:]\n",
    "x_train_cbow, x_val_cbow, x_test_cbow = x_train_cbow[:val], x_train_cbow[val:test], x_train_cbow[test:]\n",
    "x_train_sg, x_val_sg, x_test_sg = x_train_sg[:val], x_train_sg[val:test], x_train_sg[test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "rnSdid2GpBpa",
    "outputId": "ac220172-4f5b-4777-8e5f-79818468909d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3770,)\n",
      "(5466, 100)\n",
      "(18846,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_test))\n",
    "print(np.shape(x_test_cbow))\n",
    "print(np.shape(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcxyoeOUnguT"
   },
   "source": [
    "*    Plot the loss vs iteration curve, classification error vs iteration curve, classification accuracy vs iteration curve for training data and report your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "colab_type": "code",
    "id": "heU8zSV8SgCx",
    "outputId": "ea89f3aa-1b38-4d4a-b429-2035b6e6a6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t1.5\t0.499\t0.498\n",
      "10\t1.4\t0.548\t0.538\n",
      "15\t1.3\t0.575\t0.556\n",
      "20\t1.3\t0.591\t0.566\n",
      "25\t1.2\t0.602\t0.578\n",
      "30\t1.2\t0.611\t0.586\n",
      "35\t1.2\t0.617\t0.593\n",
      "40\t1.2\t0.622\t0.596\n",
      "45\t1.1\t0.627\t0.604\n",
      "50\t1.1\t0.631\t0.607\n",
      "55\t1.1\t0.634\t0.611\n",
      "60\t1.1\t0.638\t0.615\n",
      "65\t1.1\t0.641\t0.617\n",
      "70\t1.1\t0.644\t0.622\n",
      "75\t1.1\t0.646\t0.624\n",
      "80\t1.1\t0.648\t0.622\n",
      "85\t1.1\t0.651\t0.624\n",
      "90\t1.1\t0.654\t0.623\n",
      "95\t1.0\t0.657\t0.62\n",
      "100\t1.0\t0.659\t0.619\n",
      "105\t1.0\t0.66\t0.62\n",
      "110\t1.0\t0.662\t0.623\n",
      "115\t1.0\t0.665\t0.624\n",
      "120\t1.0\t0.666\t0.625\n",
      "125\t1.0\t0.669\t0.624\n",
      "130\t1.0\t0.67\t0.626\n",
      "135\t1.0\t0.671\t0.627\n",
      "140\t1.0\t0.673\t0.629\n",
      "145\t1.0\t0.676\t0.629\n",
      "150\t1.0\t0.677\t0.633\n",
      "155\t1.0\t0.679\t0.633\n",
      "160\t1.0\t0.682\t0.63\n",
      "165\t1.0\t0.682\t0.631\n",
      "170\t1.0\t0.684\t0.633\n",
      "175\t1.0\t0.685\t0.635\n",
      "180\t0.9\t0.686\t0.633\n",
      "185\t0.9\t0.687\t0.632\n",
      "190\t0.9\t0.689\t0.632\n",
      "195\t0.9\t0.69\t0.632\n",
      "200\t0.9\t0.692\t0.634\n",
      "\n",
      "Total training time: 165.16 seconds\n",
      "Accuracy on test data:  0.628\n"
     ]
    }
   ],
   "source": [
    "# using FNN for classification for CBOW word2vec\n",
    "# setting up parameters\n",
    "x_size = 100         # size of input vector\n",
    "h_size = 64          # size of hidden layer\n",
    "y_size = 20          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 200   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train_cbow)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])      # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])                # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, y_size])      # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train_cbow[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train_cbow, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train_cbow, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val_cbow, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test_cbow, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "colab_type": "code",
    "id": "mOMG5EBYwu1E",
    "outputId": "8aaf77ad-f78e-44da-b3e9-60697512569f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t1.4\t0.544\t0.538\n",
      "10\t1.2\t0.63\t0.621\n",
      "15\t1.0\t0.664\t0.656\n",
      "20\t1.0\t0.689\t0.67\n",
      "25\t0.9\t0.706\t0.682\n",
      "30\t0.9\t0.718\t0.691\n",
      "35\t0.9\t0.725\t0.698\n",
      "40\t0.9\t0.732\t0.705\n",
      "45\t0.8\t0.738\t0.712\n",
      "50\t0.8\t0.743\t0.718\n",
      "55\t0.8\t0.746\t0.723\n",
      "60\t0.8\t0.75\t0.723\n",
      "65\t0.8\t0.753\t0.727\n",
      "70\t0.8\t0.756\t0.73\n",
      "75\t0.8\t0.76\t0.729\n",
      "80\t0.8\t0.762\t0.732\n",
      "85\t0.8\t0.764\t0.735\n",
      "90\t0.7\t0.766\t0.736\n",
      "95\t0.7\t0.766\t0.738\n",
      "100\t0.7\t0.768\t0.741\n",
      "105\t0.7\t0.769\t0.745\n",
      "110\t0.7\t0.769\t0.747\n",
      "115\t0.7\t0.77\t0.748\n",
      "120\t0.7\t0.773\t0.749\n",
      "125\t0.7\t0.774\t0.751\n",
      "130\t0.7\t0.774\t0.75\n",
      "135\t0.7\t0.775\t0.751\n",
      "140\t0.7\t0.777\t0.751\n",
      "145\t0.7\t0.778\t0.753\n",
      "150\t0.7\t0.78\t0.752\n",
      "155\t0.7\t0.78\t0.752\n",
      "160\t0.7\t0.78\t0.753\n",
      "165\t0.7\t0.782\t0.752\n",
      "170\t0.7\t0.783\t0.753\n",
      "175\t0.7\t0.783\t0.752\n",
      "180\t0.7\t0.785\t0.754\n",
      "185\t0.7\t0.785\t0.754\n",
      "190\t0.7\t0.785\t0.756\n",
      "195\t0.7\t0.786\t0.755\n",
      "200\t0.7\t0.787\t0.755\n",
      "\n",
      "Total training time: 153.96 seconds\n",
      "Accuracy on test data:  0.737\n"
     ]
    }
   ],
   "source": [
    "# using FNN for classification for CBOW word2vec\n",
    "# setting up parameters\n",
    "x_size = 100         # size of input vector\n",
    "h_size = 64          # size of hidden layer\n",
    "y_size = 20          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 200   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train_sg)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])      # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])                # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, y_size])      # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train_sg[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train_sg, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train_sg, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val_sg, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test_sg, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtYcuJuNoDT5"
   },
   "source": [
    "*    Find the classification accuracy, the number of true positives, true negatives, false positives and false negatives for both training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfhBvRV3oKVU"
   },
   "source": [
    "*    There are two training algorithms for Word2Vec: skip-gram and bag of words. Which\n",
    "training algorithm is performing better in this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyxmhilDgnaJ"
   },
   "source": [
    "## Ques - 2\n",
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3ydNwLugs-j"
   },
   "source": [
    "*    MNIST is a database of hand written images. Download MNIST data using the built-in functions in Tensorflow or Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-njzEILDsflg"
   },
   "outputs": [],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7EyI5RGtBIC"
   },
   "source": [
    "*    Split the training data into training data and validation data of size 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQioXKGSsjH6"
   },
   "outputs": [],
   "source": [
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEbYJBvYskFz"
   },
   "source": [
    "*    Create a fully connected feed forward neural network for MNIST classification with one hidden layer(32 nodes). Train the model using Stochastic Gradient Descent optimizer with learning rate 0.1. Use Sigmoid activation function in the hidden layer.\n",
    "\n",
    "#### Training with unnormalized data\n",
    "Learning rate = 0.1 <br>\n",
    "No of iterations = 100 <br>\n",
    "No of hidden layers = 1 <br>\n",
    "No of nodes in the hidden layer = 32 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "KARlCWFruUti",
    "outputId": "3801b28b-ccc3-435f-f2b5-ad39729cb55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.8\t0.724\t0.762\n",
      "10\t0.7\t0.792\t0.835\n",
      "15\t0.6\t0.817\t0.85\n",
      "20\t0.6\t0.817\t0.849\n",
      "25\t0.5\t0.845\t0.874\n",
      "30\t0.5\t0.848\t0.872\n",
      "35\t0.5\t0.861\t0.886\n",
      "40\t0.4\t0.866\t0.891\n",
      "45\t0.4\t0.866\t0.887\n",
      "50\t0.4\t0.87\t0.891\n",
      "55\t0.4\t0.865\t0.881\n",
      "60\t0.4\t0.88\t0.898\n",
      "65\t0.4\t0.878\t0.896\n",
      "70\t0.4\t0.889\t0.909\n",
      "75\t0.4\t0.891\t0.907\n",
      "80\t0.4\t0.887\t0.9\n",
      "85\t0.4\t0.888\t0.904\n",
      "90\t0.4\t0.887\t0.906\n",
      "95\t0.4\t0.886\t0.905\n",
      "100\t0.4\t0.892\t0.91\n",
      "\n",
      "Total training time: 82.12 seconds\n",
      "Accuracy on test data:  0.9\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 32          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)     # hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H, w_2) + b_2                    # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekpRaBrxu62N"
   },
   "source": [
    "*    Normalize the dataset to range (0,1). Compare both the normalized and unnormalized\n",
    "models in terms of training time and accuracy.\n",
    "\n",
    "#### Training with normalized data\n",
    "Learning rate = 0.1 <br>\n",
    "No of iterations = 100 <br>\n",
    "No of hidden layers = 1 <br>\n",
    "No of nodes in the hidden layer = 32 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "auJMyGBt8pH5",
    "outputId": "fee6d6d5-a33b-4563-98fb-338754b1b966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.6\t0.813\t0.844\n",
      "10\t0.5\t0.858\t0.883\n",
      "15\t0.4\t0.877\t0.899\n",
      "20\t0.4\t0.89\t0.907\n",
      "25\t0.3\t0.898\t0.914\n",
      "30\t0.3\t0.905\t0.918\n",
      "35\t0.3\t0.91\t0.924\n",
      "40\t0.3\t0.915\t0.926\n",
      "45\t0.3\t0.919\t0.929\n",
      "50\t0.3\t0.922\t0.931\n",
      "55\t0.3\t0.925\t0.932\n",
      "60\t0.2\t0.927\t0.933\n",
      "65\t0.2\t0.93\t0.936\n",
      "70\t0.2\t0.932\t0.938\n",
      "75\t0.2\t0.934\t0.938\n",
      "80\t0.2\t0.936\t0.94\n",
      "85\t0.2\t0.937\t0.942\n",
      "90\t0.2\t0.939\t0.942\n",
      "95\t0.2\t0.94\t0.943\n",
      "100\t0.2\t0.941\t0.945\n",
      "\n",
      "Total training time: 90.52 seconds\n",
      "Accuracy on test data:  0.928\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 32          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)     # hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H, w_2) + b_2                    # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6z2s-DmAyBAb"
   },
   "source": [
    "#### Comparison between model with normalized data vs unnormalized data\n",
    "As clear with the accuracy on the test data, the model with normalized data performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHgbCAkAz3am"
   },
   "source": [
    "*  Choose the best performing model till now. <br>\n",
    "Train different models by varying\n",
    "the number of hidden layers in the model as 2 and 3. Record the observations.\n",
    "\n",
    "#### Training with normalized data\n",
    "Learning rate = 0.1 <br>\n",
    "No of iterations = 100 <br>\n",
    "No of hidden layers = 2 <br>\n",
    "No of nodes in the hidden layer = 32 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "yxx-X1UlxUwZ",
    "outputId": "9232fdaf-3f64-49eb-fc3f-f886542f31e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.7\t0.773\t0.805\n",
      "10\t0.5\t0.84\t0.87\n",
      "15\t0.4\t0.87\t0.891\n",
      "20\t0.4\t0.886\t0.902\n",
      "25\t0.3\t0.896\t0.911\n",
      "30\t0.3\t0.905\t0.915\n",
      "35\t0.3\t0.911\t0.918\n",
      "40\t0.3\t0.917\t0.921\n",
      "45\t0.3\t0.922\t0.925\n",
      "50\t0.2\t0.926\t0.928\n",
      "55\t0.2\t0.929\t0.93\n",
      "60\t0.2\t0.932\t0.932\n",
      "65\t0.2\t0.935\t0.935\n",
      "70\t0.2\t0.937\t0.936\n",
      "75\t0.2\t0.94\t0.936\n",
      "80\t0.2\t0.942\t0.94\n",
      "85\t0.2\t0.944\t0.941\n",
      "90\t0.2\t0.946\t0.942\n",
      "95\t0.2\t0.947\t0.942\n",
      "100\t0.2\t0.949\t0.942\n",
      "\n",
      "Total training time: 99.31 seconds\n",
      "Accuracy on test data:  0.933\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 32          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIfSqPcy1Z3e"
   },
   "source": [
    "#### Training with normalized data\n",
    "Learning rate = 0.1 <br>\n",
    "No of iterations = 100 <br>\n",
    "No of hidden layers = 3 <br>\n",
    "No of nodes in the hidden layer = 32 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "0Z7JDsDl1HW9",
    "outputId": "0bc240d3-d84e-4a0b-b8c2-32ff4bf0c38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.7\t0.788\t0.819\n",
      "10\t0.5\t0.849\t0.871\n",
      "15\t0.4\t0.874\t0.89\n",
      "20\t0.4\t0.889\t0.901\n",
      "25\t0.3\t0.899\t0.909\n",
      "30\t0.3\t0.907\t0.913\n",
      "35\t0.3\t0.913\t0.918\n",
      "40\t0.3\t0.918\t0.922\n",
      "45\t0.3\t0.922\t0.926\n",
      "50\t0.2\t0.925\t0.93\n",
      "55\t0.2\t0.928\t0.933\n",
      "60\t0.2\t0.932\t0.936\n",
      "65\t0.2\t0.935\t0.938\n",
      "70\t0.2\t0.937\t0.939\n",
      "75\t0.2\t0.939\t0.941\n",
      "80\t0.2\t0.941\t0.943\n",
      "85\t0.2\t0.943\t0.944\n",
      "90\t0.2\t0.944\t0.945\n",
      "95\t0.2\t0.946\t0.946\n",
      "100\t0.2\t0.947\t0.947\n",
      "\n",
      "Total training time: 105.53 seconds\n",
      "Accuracy on test data:  0.925\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 32          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_4 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_4 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "H3 = tf.nn.sigmoid(tf.matmul(H2, w_3) + b_3)   # third hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H3, w_4) + b_4                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5urKrdM71hoH"
   },
   "source": [
    "#### Comparison between model with normalized data having 1, 2, 3 hidden layers\n",
    "From the results in the previous three models we find that the model with 3 hidden layers performs better than the model with 1 or 2 hidden layer, on the validation data. However, when comparing the results for test data, the second model with 2 hidden layers is better. This suggests that the third model (with three hidden layers), is so complex that it overfits the training data. \n",
    "So, to conclude the second model is better for our purpose of classifying hand written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BabQ3Tcl3pzd"
   },
   "source": [
    "*    Choose the best performing model till now. <br>\n",
    "Train models by varying the learning rates as 0.001 and 0.0001 and record your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1JSmOtN4BFW"
   },
   "source": [
    "#### Training with normalized data and two hidden layers\n",
    "Learning rate = 0.001 <br>\n",
    "No of iterations = 100 <br>\n",
    "No of nodes in the hidden layer = 32 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "z_pPQ4vL4ONw",
    "outputId": "051b8f73-48f8-4ffd-9eec-0a124eca35d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t2.8\t0.099\t0.105\n",
      "10\t2.4\t0.163\t0.167\n",
      "15\t2.3\t0.208\t0.211\n",
      "20\t2.1\t0.246\t0.245\n",
      "25\t2.0\t0.278\t0.278\n",
      "30\t2.0\t0.307\t0.313\n",
      "35\t1.9\t0.335\t0.341\n",
      "40\t1.8\t0.362\t0.369\n",
      "45\t1.8\t0.388\t0.399\n",
      "50\t1.7\t0.412\t0.422\n",
      "55\t1.7\t0.434\t0.446\n",
      "60\t1.6\t0.453\t0.468\n",
      "65\t1.6\t0.471\t0.485\n",
      "70\t1.6\t0.487\t0.505\n",
      "75\t1.5\t0.502\t0.524\n",
      "80\t1.5\t0.516\t0.537\n",
      "85\t1.4\t0.528\t0.55\n",
      "90\t1.4\t0.541\t0.563\n",
      "95\t1.4\t0.553\t0.574\n",
      "100\t1.4\t0.562\t0.584\n",
      "\n",
      "Total training time: 100.06 seconds\n",
      "Accuracy on test data:  0.571\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 32          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.001   # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Kuq1jFV4Dn6"
   },
   "source": [
    "#### Training with normalized data and two hidden layers\n",
    "Learning rate = 0.0001 <br>\n",
    "No of iterations = 100 <br>\n",
    "No of nodes in the hidden layer = 32 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "tiKQ-rvN4TNs",
    "outputId": "4ad9afb2-9bcd-4f33-8b56-125d03cbeb9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t4.3\t0.119\t0.124\n",
      "10\t3.8\t0.112\t0.116\n",
      "15\t3.5\t0.104\t0.106\n",
      "20\t3.3\t0.098\t0.096\n",
      "25\t3.1\t0.096\t0.093\n",
      "30\t3.0\t0.096\t0.094\n",
      "35\t2.9\t0.096\t0.094\n",
      "40\t2.8\t0.097\t0.096\n",
      "45\t2.8\t0.098\t0.097\n",
      "50\t2.7\t0.101\t0.099\n",
      "55\t2.7\t0.103\t0.1\n",
      "60\t2.7\t0.105\t0.101\n",
      "65\t2.7\t0.107\t0.103\n",
      "70\t2.6\t0.109\t0.109\n",
      "75\t2.6\t0.111\t0.111\n",
      "80\t2.6\t0.114\t0.114\n",
      "85\t2.6\t0.117\t0.117\n",
      "90\t2.5\t0.119\t0.119\n",
      "95\t2.5\t0.122\t0.122\n",
      "100\t2.5\t0.125\t0.125\n",
      "\n",
      "Total training time: 101.67 seconds\n",
      "Accuracy on test data:  0.117\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 32          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.0001  # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zahLu_cc4xCU"
   },
   "source": [
    "#### Comparison between models with learning rates of 0.1, 0.001, 0.0001\n",
    "Although theoretically learning rates 0.001, 0.0001 are better compared to 0.1 as they provide smooth movement towards the convergence point, their convergence is too slow. So, from practical point of view the model with 0.1 learning rate is better if we fix the no of iterations for the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUVpIJNt5wjh"
   },
   "source": [
    "*    Choose the best performing model till now. <br> \n",
    "Train models by varying the number of nodes in each hidden layer to 64 and 128.\n",
    "\n",
    "#### Training with normalized data and two hidden layers, learning rate = 0.1\n",
    "No of iterations = 100 <br>\n",
    "No of nodes in the hidden layer = 64 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "PLM71scR6Cad",
    "outputId": "496bcc4f-69aa-4b20-d75f-6f62ef4d65b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.5\t0.829\t0.854\n",
      "10\t0.4\t0.874\t0.891\n",
      "15\t0.3\t0.894\t0.906\n",
      "20\t0.3\t0.906\t0.916\n",
      "25\t0.3\t0.914\t0.92\n",
      "30\t0.3\t0.922\t0.925\n",
      "35\t0.2\t0.927\t0.929\n",
      "40\t0.2\t0.932\t0.932\n",
      "45\t0.2\t0.935\t0.935\n",
      "50\t0.2\t0.939\t0.936\n",
      "55\t0.2\t0.941\t0.937\n",
      "60\t0.2\t0.944\t0.938\n",
      "65\t0.2\t0.947\t0.94\n",
      "70\t0.2\t0.949\t0.942\n",
      "75\t0.2\t0.951\t0.943\n",
      "80\t0.2\t0.954\t0.944\n",
      "85\t0.1\t0.955\t0.944\n",
      "90\t0.1\t0.957\t0.944\n",
      "95\t0.1\t0.959\t0.945\n",
      "100\t0.1\t0.961\t0.947\n",
      "\n",
      "Total training time: 124.19 seconds\n",
      "Accuracy on test data:  0.935\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 64          # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "frt5jNfI5_QW"
   },
   "source": [
    "#### Training with normalized data and two hidden layers, learning rate = 0.1\n",
    "No of iterations = 100 <br>\n",
    "No of nodes in the hidden layer = 128 <br>\n",
    "Activation function = Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "0wdAahiK6LZO",
    "outputId": "91ac7132-c430-4b65-9815-73ab1461f42d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.5\t0.848\t0.867\n",
      "10\t0.4\t0.887\t0.897\n",
      "15\t0.3\t0.906\t0.913\n",
      "20\t0.3\t0.918\t0.925\n",
      "25\t0.2\t0.927\t0.927\n",
      "30\t0.2\t0.934\t0.93\n",
      "35\t0.2\t0.939\t0.934\n",
      "40\t0.2\t0.944\t0.936\n",
      "45\t0.2\t0.948\t0.939\n",
      "50\t0.2\t0.951\t0.94\n",
      "55\t0.2\t0.954\t0.942\n",
      "60\t0.1\t0.957\t0.942\n",
      "65\t0.1\t0.96\t0.942\n",
      "70\t0.1\t0.963\t0.942\n",
      "75\t0.1\t0.964\t0.944\n",
      "80\t0.1\t0.966\t0.945\n",
      "85\t0.1\t0.968\t0.946\n",
      "90\t0.1\t0.97\t0.946\n",
      "95\t0.1\t0.971\t0.946\n",
      "100\t0.1\t0.973\t0.947\n",
      "\n",
      "Total training time: 188.52 seconds\n",
      "Accuracy on test data:  0.934\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 128         # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rGdTkL57oEt"
   },
   "source": [
    "#### Comparison between models with 32, 64, 128 nodes in the hidden layer\n",
    "From the accuracy results obtained above, we find that the model with 128 nodes in the hidden layer performs better, which is also expected because of increased complexity of the network which enhances its respresentation ability. <br> \n",
    "Also, another thing to notice is that the training time for the same number of iterations also increases considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bf9ackf68e3h"
   },
   "source": [
    "*    Choose the best performing model till now. <br>\n",
    "Train models by varying the activation functions in each of the hidden layers to tanh, ReLU and leaky ReLU and record your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMS60Tiv86se"
   },
   "source": [
    "#### Training with normalized data and two hidden layers, learning rate = 0.1 and 128 nodes in the hidden layer\n",
    "No of iterations = 100 <br>\n",
    "Activation function = tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "-OcP6qKF9EyB",
    "outputId": "6db72946-785d-462b-c1d2-9266f1370b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t0.5\t0.869\t0.862\n",
      "10\t0.3\t0.903\t0.878\n",
      "15\t0.3\t0.921\t0.888\n",
      "20\t0.2\t0.933\t0.891\n",
      "25\t0.2\t0.942\t0.897\n",
      "30\t0.2\t0.95\t0.896\n",
      "35\t0.2\t0.956\t0.897\n",
      "40\t0.1\t0.961\t0.898\n",
      "45\t0.1\t0.965\t0.898\n",
      "50\t0.1\t0.968\t0.898\n",
      "55\t0.1\t0.971\t0.901\n",
      "60\t0.1\t0.974\t0.903\n",
      "65\t0.1\t0.977\t0.903\n",
      "70\t0.1\t0.979\t0.903\n",
      "75\t0.1\t0.981\t0.903\n",
      "80\t0.1\t0.983\t0.905\n",
      "85\t0.1\t0.985\t0.904\n",
      "90\t0.1\t0.987\t0.902\n",
      "95\t0.1\t0.988\t0.903\n",
      "100\t0.0\t0.989\t0.904\n",
      "\n",
      "Total training time: 187.81 seconds\n",
      "Accuracy on test data:  0.887\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 128         # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.tanh(tf.matmul(X, w_1) + b_1)       # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.tanh(tf.matmul(H1, w_2) + b_2)      # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoAReJDb8vsf"
   },
   "source": [
    "#### Training with normalized data and two hidden layers, learning rate = 0.1 and 128 nodes in the hidden layer\n",
    "No of iterations = 100 <br>\n",
    "Activation function = ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "jq1AC1tG9Zet",
    "outputId": "8da37b7e-e920-4f44-b03f-e6d706276033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\t1.7\t0.305\t0.317\n",
      "10\t1.5\t0.369\t0.375\n",
      "15\t1.4\t0.452\t0.46\n",
      "20\t1.1\t0.592\t0.622\n",
      "25\t0.8\t0.706\t0.734\n",
      "30\t0.7\t0.737\t0.767\n",
      "35\t0.7\t0.773\t0.797\n",
      "40\t0.6\t0.823\t0.847\n",
      "45\t0.5\t0.857\t0.877\n",
      "50\t0.4\t0.873\t0.893\n",
      "55\t0.4\t0.888\t0.903\n",
      "60\t0.4\t0.898\t0.909\n",
      "65\t0.3\t0.904\t0.914\n",
      "70\t0.3\t0.908\t0.916\n",
      "75\t0.3\t0.913\t0.921\n",
      "80\t0.3\t0.917\t0.923\n",
      "85\t0.3\t0.921\t0.923\n",
      "90\t0.3\t0.924\t0.925\n",
      "95\t0.3\t0.927\t0.927\n",
      "100\t0.2\t0.929\t0.929\n",
      "\n",
      "Total training time: 191.39 seconds\n",
      "Accuracy on test data:  0.912\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 128         # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.relu(tf.matmul(X, w_1) + b_1)       # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.relu(tf.matmul(H1, w_2) + b_2)      # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68dolYlM8wL0"
   },
   "source": [
    "#### Training with normalized data and two hidden layers, learning rate = 0.1, 128 nodes in the hidden layer\n",
    "No of iterations = 100 <br>\n",
    "Activation function = Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "B5fTOC7J9irw",
    "outputId": "d2301ad1-2392-4893-83e4-189aa828886a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr\tloss\tacc_t\tacc_v\n",
      "5\tnan\t0.0\t0.0\n",
      "10\tnan\t0.0\t0.0\n",
      "15\tnan\t0.0\t0.0\n",
      "20\tnan\t0.0\t0.0\n",
      "25\tnan\t0.0\t0.0\n",
      "30\tnan\t0.0\t0.0\n",
      "35\tnan\t0.0\t0.0\n",
      "40\tnan\t0.0\t0.0\n",
      "45\tnan\t0.0\t0.0\n",
      "50\tnan\t0.0\t0.0\n",
      "55\tnan\t0.0\t0.0\n",
      "60\tnan\t0.0\t0.0\n",
      "65\tnan\t0.0\t0.0\n",
      "70\tnan\t0.0\t0.0\n",
      "75\tnan\t0.0\t0.0\n",
      "80\tnan\t0.0\t0.0\n",
      "85\tnan\t0.0\t0.0\n",
      "90\tnan\t0.0\t0.0\n",
      "95\tnan\t0.0\t0.0\n",
      "100\tnan\t0.0\t0.0\n",
      "\n",
      "Total training time: 196.21 seconds\n",
      "Accuracy on test data:  0.0\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 128         # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 100   # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.leaky_relu(tf.matmul(X, w_1) + b_1, alpha=0.01) # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.leaky_relu(tf.matmul(H1, w_2) + b_2, alpha=0.01)# second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                               # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 5 == 4:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFZ3Qg2eCzMG"
   },
   "source": [
    "#### Comparison between models with sigmoid, tanh, ReLU and Leaky ReLU activation functions\n",
    "The model with **tanh** activation function overfits the training data and hence is not able to perform better on the test data. Among the remaining three sigmoid function is better as apparent from the accuracy results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw1Yii6ODUGW"
   },
   "source": [
    "*     Among all the configurations of hyper-parameters that you trained above, which setting is best. How did you decide which setting is better ?\n",
    "\n",
    "Two models can be compared by their performance on the test data for fixed number of iterations of gradient descent. The one with higher accuracy is better. Best on this consideration, the best hyper-parameters settings obtained are - <br>\n",
    "**(un)normalized data** - normalized data <br>\n",
    "**number of hidden layers** - 2 <br>\n",
    "**learning rate** - 0.1\n",
    "**number of nodes in hidden layer** - 128 <br>\n",
    "**activation function** - Sigmoid <br>\n",
    "\n",
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "bbPtudHVF3KP",
    "outputId": "d17dcfc6-f174-4e6b-de7a-579f654997e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "itr\tloss\tacc_t\tacc_v\n",
      "100\t0.1\t0.976\t0.949\n",
      "200\t0.0\t0.993\t0.953\n",
      "300\t0.0\t0.998\t0.956\n",
      "400\t0.0\t1.0\t0.957\n",
      "500\t0.0\t1.0\t0.958\n",
      "\n",
      "Total training time: 1008.32 seconds\n",
      "Accuracy on test data:  0.95\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# splitting the training data into training and validation data\n",
    "x_train, x_val = x_train[:-5000][:][:], x_train[-5000:][:][:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# resizing the image matrices into a linear vector\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "# normalizing the data\n",
    "x_train = x_train / 256\n",
    "x_val = x_val / 256\n",
    "x_test = x_test / 256\n",
    "\n",
    "# setting up parameters\n",
    "x_size = 784         # size of input vector (28 * 28 = 784)\n",
    "h_size = 128         # size of hidden layer\n",
    "y_size = 10          # size of output vector\n",
    "learn_rate = 0.1     # learning rate for gradient descent\n",
    "n_iterations = 500  # no of iterations of gradient descent to be performed\n",
    "n_batches = 500      # no of batches for training\n",
    "batch_size = int(len(x_train)/n_batches) # size of each batch\n",
    "\n",
    "# inputs and outputs\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, x_size])    # input\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None])              # actual output\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10])          # predicted output\n",
    "\n",
    "# weight and bias tensors\n",
    "w_1 = tf.Variable(tf.random_normal((x_size, h_size), stddev=1))\n",
    "b_1 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_2 = tf.Variable(tf.random_normal((h_size, h_size), stddev=1))\n",
    "b_2 = tf.Variable(tf.random_normal([h_size], stddev=1))\n",
    "w_3 = tf.Variable(tf.random_normal((h_size, y_size), stddev=1))\n",
    "b_3 = tf.Variable(tf.random_normal([y_size], stddev=1))\n",
    "\n",
    "# forward propagation\n",
    "H1 = tf.nn.sigmoid(tf.matmul(X, w_1) + b_1)    # first hidden layer with sigmoid activation function\n",
    "H2 = tf.nn.sigmoid(tf.matmul(H1, w_2) + b_2)   # second hidden layer with sigmoid activation function\n",
    "y = tf.matmul(H2, w_3) + b_3                   # output layer with no activation function\n",
    "\n",
    "# backward propagation\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=Y))    # cross entropy cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "correct = tf.nn.in_top_k(y, Y, 1)\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# running the model\n",
    "init = tf.global_variables_initializer()\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"itr\\tloss\\tacc_t\\tacc_v\")\n",
    "    \n",
    "    for iteration_id in range(n_iterations):\n",
    "        for batch_id in range(n_batches):\n",
    "            # find the current batch\n",
    "            i = batch_id * batch_size\n",
    "            j = (batch_id + 1) * batch_size\n",
    "            xx_train, yy_train = x_train[i:j][:], y_train[i:j]\n",
    "            \n",
    "            # train and update parameters from the current batch data\n",
    "            sess.run(optimizer, feed_dict={X:xx_train, Y:yy_train})\n",
    "        \n",
    "        # perform evaluation at the end of each iteration\n",
    "        loss_val = sess.run([cost], feed_dict={X:x_train, Y:y_train})\n",
    "        \n",
    "        # evaluate accuracy on training/validation data\n",
    "        acc_train = sess.run([acc], feed_dict={X:x_train, Y:y_train})\n",
    "        acc_val = sess.run([acc], feed_dict={X:x_val, Y:y_val})\n",
    "        \n",
    "        # printing training statistics\n",
    "        if iteration_id % 100 == 99:\n",
    "            print('{}\\t{}\\t{}\\t{}'.format(iteration_id + 1, str(round(loss_val[0], 1)), \n",
    "                  str(round(acc_train[0], 3)), str(round(acc_val[0], 3))))\n",
    "\n",
    "    # printing the training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\nTotal training time: %.2f seconds\" % (end_time - start_time))\n",
    "\n",
    "    # running the model on test data\n",
    "    acc_test = sess.run([acc], feed_dict={X:x_test, Y:y_test})\n",
    "    print(\"Accuracy on test data: \", str(round(acc_test[0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWXvBvyzEb9K"
   },
   "source": [
    "*    Among all the models trained above, how will you choose the best model? Which is the best model?\n",
    "\n",
    "There is no hard and fast rule for choosing the best model for a given problem. We must vary the model parameters and check the accuracy on the validation data. The model that performs the best on validation data is the best. Varying the parameters for a feedforward neural network like - no. of hidden layers, no. of nodes in the hidden layer, learning rate etc., we obtained the above configuration of parameters which performs the best."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
