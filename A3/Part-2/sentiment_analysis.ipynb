{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# !cp \"gdrive/My Drive/assignments\" -r saved_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 150\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "                                                   X  Y\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...  1\n",
      "1  Homelessness (or Houselessness as George Carli...  1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...  1\n",
      "3  This is easily the most underrated film inn th...  1\n",
      "4  This is not the typical Mel Brooks film. It wa...  1\n",
      "                                                       X  Y\n",
      "24995  I occasionally let my kids watch this garbage ...  0\n",
      "24996  When all we have anymore is pretty much realit...  0\n",
      "24997  The basic genre is a thriller intercut with an...  0\n",
      "24998  Four things intrigued me as to this film - fir...  0\n",
      "24999  David Bryce's comments nearby are exceptionall...  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z]{2,}')\n",
    "train_df = pd.read_csv('data/train.txt', delimiter='\\n', header=None, names=['X'])\n",
    "test_df = pd.read_csv('data/test.txt', delimiter='\\n', header=None, names=['X'])\n",
    "\n",
    "col_y = ([1] * 12500) + ([0] * 12500)\n",
    "train_df.insert (1, 'Y', col_y, True)\n",
    "test_df.insert (1, 'Y', col_y, True)\n",
    "\n",
    "print (len(train_df))\n",
    "print (len(test_df))\n",
    "print (train_df.head())\n",
    "print (test_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   X  Y\n",
      "0  bromwell high is cartoon comedy it ran at the ...  1\n",
      "1  homelessness or houselessness as george carlin...  1\n",
      "2  brilliant over acting by lesley ann warren bes...  1\n",
      "3  this is easily the most underrated film inn th...  1\n",
      "4  this is not the typical mel brooks film it was...  1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z]{2,}')\n",
    "\n",
    "def preprocess (text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return (\" \".join(tokens))\n",
    "    \n",
    "train_df['X'] = train_df.apply(lambda row: preprocess(row['X']), axis=1)\n",
    "test_df['X'] = test_df.apply(lambda row: preprocess(row['X']), axis=1)\n",
    "print (train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   X  Y\n",
      "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  1\n",
      "1  [87, 88, 89, 12, 90, 91, 92, 93, 94, 95, 96, 9...  1\n",
      "2  [287, 288, 289, 203, 214, 215, 216, 290, 291, ...  1\n",
      "3  [282, 3, 356, 9, 116, 357, 283, 358, 9, 164, 3...  1\n",
      "4  [282, 3, 236, 9, 405, 163, 164, 283, 6, 406, 3...  1\n",
      "Sample padded sequence: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 12, 20, 21, 22, 23, 9, 24, 25, 26, 27, 28, 29, 30, 1, 2, 31, 3, 32, 33, 28, 34, 35, 3, 20, 9, 36, 28, 37, 38, 9, 39, 40, 41, 42, 43, 44, 45, 46, 47, 20, 48, 9, 49, 50, 9, 51, 52, 53, 54, 27, 50, 9, 55, 56, 57, 46, 40, 58, 59, 9, 60, 23, 61, 62, 63, 64, 28, 65, 66, 9, 17, 67, 68, 8, 2, 69, 70, 71, 72, 28, 73, 74, 50, 75, 20, 62, 76, 28, 1, 2, 77, 30, 78, 79, 50, 21, 80, 81, 30, 1, 2, 3, 82, 83, 84, 85, 30, 6, 86, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "vocab_size: 99401\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoded_train_df = train_df.copy(deep=True)\n",
    "encoded_test_df = test_df.copy(deep=True)\n",
    "\n",
    "vocab = {}\n",
    "k = 1\n",
    "\n",
    "def encode (text):\n",
    "    global k\n",
    "    words = text.split()\n",
    "    code = []\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = k\n",
    "            k += 1\n",
    "        code.append(vocab[word])\n",
    "\n",
    "    if len(code) > 200:\n",
    "        code = code[:200]\n",
    "    code = code + [0] * (200 - len(code))\n",
    "    return code\n",
    "\n",
    "encoded_train_df['X'] = encoded_train_df.apply(lambda row: encode(row['X']), axis=1)\n",
    "encoded_test_df['X'] = encoded_test_df.apply(lambda row: encode(row['X']), axis=1)\n",
    "\n",
    "print (encoded_train_df.head())\n",
    "\n",
    "sample = encoded_train_df.iloc[0]['X']\n",
    "print ('Sample padded sequence:', sample)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print ('vocab_size:', vocab_size)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "encoded_train_df, encoded_val_df = train_test_split(encoded_train_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = encoded_train_df['X'].tolist()\n",
    "train_y = encoded_train_df['Y'].tolist()\n",
    "val_x = encoded_val_df['X'].tolist()\n",
    "val_y = encoded_val_df['Y'].tolist()\n",
    "test_x = encoded_test_df['X'].tolist()\n",
    "val_y = encoded_test_df['Y'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and compiling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 128)          12800000  \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 200)               65800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 12,866,001\n",
      "Trainable params: 12,866,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "models = [0] * 6\n",
    "models[0] = Sequential([\n",
    "    layers.Embedding(100000, 128, input_length=200),\n",
    "    layers.SimpleRNN(200, activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "print (models[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 128)          12800000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 13,063,401\n",
      "Trainable params: 13,063,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "models[1] = Sequential([\n",
    "    layers.Embedding(100000, 128, input_length=200),\n",
    "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "print (models[1].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 128)          12800000  \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 200)               198000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 12,998,201\n",
      "Trainable params: 12,998,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "models[2] = Sequential([\n",
    "    layers.Embedding(100000, 128, input_length=200),\n",
    "    layers.GRU(200, activation='relu', recurrent_activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "print (models[2].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(1e-4)\n",
    "for i in range(3):\n",
    "    models[i].compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, \n",
    "                           verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "callbacks_list = [early_stop]\n",
    "hist_temp = 'keras_imdb_history{}.pkl'\n",
    "hist_names = [hist_temp.format(x) for x in range(1, 7)]\n",
    "template = 'keras_imdb_trained_model{}.h5'\n",
    "model_names = [template.format(x) for x in range(1, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-13-7a5f03d00746>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (os.path.exists(model_path), 'Model must be saved at model_path')\n"
     ]
    }
   ],
   "source": [
    "def save_history (hist_path, history):\n",
    "    print ('Saving history at', hist_path, flush=True)\n",
    "    file_object = open (hist_path, 'wb')\n",
    "    pickle.dump (history, file_object)\n",
    "    file_object.close()\n",
    "    \n",
    "def load_history (hist_path):\n",
    "    print ('Loading history from', hist_path, flush=True)\n",
    "    file_object = open(hist_path, 'rb')\n",
    "    history = pickle.load(file_object)\n",
    "    file_object.close()\n",
    "    return history\n",
    "\n",
    "def train_and_evaluate(model_id):\n",
    "    model = models[model_id]\n",
    "    model_path = os.path.join(save_dir, model_names[model_id])\n",
    "    hist_path = os.path.join(save_dir, hist_names[model_id])\n",
    "    if os.path.exists(hist_path):\n",
    "        assert (os.path.exists(model_path), 'Model must be saved at model_path')\n",
    "        history = load_history(hist_path)\n",
    "        print ('Trained model loaded from {}', model_path)\n",
    "        model = keras.models.load_model(model_path)\n",
    "        loss, accuracy = model.evaluate (test_x, test_y, verbose=0)\n",
    "        print ('Loss = {}, Accuracy = {}'.format(loss, accuracy))\n",
    "        return history, loss, accuracy\n",
    "        \n",
    "    history = model.fit(train_x, train_y, epochs=epochs, \n",
    "                         validation_data=(val_x, val_y), \n",
    "                         workers=4, shuffle=True, callbacks=callbacks_list)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(test_x, test_y, verbose=0)\n",
    "    print ('Loss = {}, Accuracy = {}'.format(loss, accuracy))\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    save_history(hist_path, history.history)\n",
    "    return history.history, loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 5000\n  y sizes: 25000\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-02cce79968e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mhistory_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     data_dict = {'test_accuracy':accuracy, \n",
      "\u001b[0;32m<ipython-input-13-7a5f03d00746>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model_id)\u001b[0m\n\u001b[1;32m     27\u001b[0m     history = model.fit(train_x, train_y, epochs=epochs, \n\u001b[1;32m     28\u001b[0m                          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                          workers=4, shuffle=True, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    561\u001b[0m                                     \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                                     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                                     distribution_strategy=distribution_strategy)\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m       raise ValueError('`validation_steps` should not be specified if '\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    603\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    606\u001b[0m   \u001b[0;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m   \u001b[0;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     self._internal_adapter = TensorLikeDataAdapter(\n\u001b[1;32m    465\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         batch_size=batch_size, shuffle=shuffle, **kwargs)\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             label, \", \".join([str(i.shape[0]) for i in nest.flatten(data)]))\n\u001b[1;32m    231\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 5000\n  y sizes: 25000\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "history_list = [0] * 6\n",
    "training_data = [0] * 6\n",
    "\n",
    "for i in range (3):\n",
    "    history, loss, accuracy = train_and_evaluate(i)\n",
    "    history_list[i] = history\n",
    "    data_dict = {'test_accuracy':accuracy, \n",
    "                 'train_accuracy':history['acc'][-1],\n",
    "                 'val_accuracy':history['val_acc'][-1]}\n",
    "    training_data[i] = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
