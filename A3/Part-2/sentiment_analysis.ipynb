{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "71w1bWX637My",
        "colab_type": "code",
        "outputId": "78e4112d-7c15-4f81-8d9f-c4c8ec27573f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !cp \"gdrive/My Drive/data\" -r ./"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njGosSPY37M8",
        "colab_type": "text"
      },
      "source": [
        "#### Importing tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo06HCfP37M9",
        "colab_type": "code",
        "outputId": "bfc32866-59b9-4147-dd26-1d6d2d878748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JGA9XiD37ND",
        "colab_type": "text"
      },
      "source": [
        "#### Setting parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KKeu3eL37NF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 150\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytVfA6Zb37NI",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4Ya92-37NJ",
        "colab_type": "code",
        "outputId": "4c66fcac-abb1-47df-ffc7-dffa447a899f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer('[a-zA-Z]{2,}')\n",
        "train_df = pd.read_csv('data/train.txt', delimiter='\\n', header=None, names=['X'])\n",
        "test_df = pd.read_csv('data/test.txt', delimiter='\\n', header=None, names=['X'])\n",
        "\n",
        "col_y = ([1] * 12500) + ([0] * 12500)\n",
        "train_df.insert (1, 'Y', col_y, True)\n",
        "test_df.insert (1, 'Y', col_y, True)\n",
        "\n",
        "print (len(train_df))\n",
        "print (len(test_df))\n",
        "print (train_df.head())\n",
        "print (test_df.tail())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n",
            "                                                   X  Y\n",
            "0  Bromwell High is a cartoon comedy. It ran at t...  1\n",
            "1  Homelessness (or Houselessness as George Carli...  1\n",
            "2  Brilliant over-acting by Lesley Ann Warren. Be...  1\n",
            "3  This is easily the most underrated film inn th...  1\n",
            "4  This is not the typical Mel Brooks film. It wa...  1\n",
            "                                                       X  Y\n",
            "24995  I occasionally let my kids watch this garbage ...  0\n",
            "24996  When all we have anymore is pretty much realit...  0\n",
            "24997  The basic genre is a thriller intercut with an...  0\n",
            "24998  Four things intrigued me as to this film - fir...  0\n",
            "24999  David Bryce's comments nearby are exceptionall...  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-zEKXSQ37NN",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbesF1CS37NO",
        "colab_type": "code",
        "outputId": "8bd28c9e-cca2-41bd-9427-a0d9d0103692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('[a-zA-Z]{2,}')\n",
        "\n",
        "def preprocess (text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return (\" \".join(tokens))\n",
        "    \n",
        "train_df['X'] = train_df.apply(lambda row: preprocess(row['X']), axis=1)\n",
        "test_df['X'] = test_df.apply(lambda row: preprocess(row['X']), axis=1)\n",
        "print (train_df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   X  Y\n",
            "0  bromwell high is cartoon comedy it ran at the ...  1\n",
            "1  homelessness or houselessness as george carlin...  1\n",
            "2  brilliant over acting by lesley ann warren bes...  1\n",
            "3  this is easily the most underrated film inn th...  1\n",
            "4  this is not the typical mel brooks film it was...  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcEoYgmR37NR",
        "colab_type": "text"
      },
      "source": [
        "#### Encode the words to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqouuNxn37NS",
        "colab_type": "code",
        "outputId": "a97037c8-3c23-47bb-c0da-37e1d5665db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoded_train_df = train_df.copy(deep=True)\n",
        "encoded_test_df = test_df.copy(deep=True)\n",
        "\n",
        "vocab = {}\n",
        "k = 1\n",
        "\n",
        "def encode (text):\n",
        "    global k\n",
        "    words = text.split()\n",
        "    code = []\n",
        "    for word in words:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = k\n",
        "            k += 1\n",
        "        code.append(vocab[word])\n",
        "\n",
        "    if len(code) > 200:\n",
        "        code = code[:200]\n",
        "    code = code + [0] * (200 - len(code))\n",
        "    return code\n",
        "\n",
        "encoded_train_df['X'] = encoded_train_df.apply(lambda row: encode(row['X']), axis=1)\n",
        "encoded_test_df['X'] = encoded_test_df.apply(lambda row: encode(row['X']), axis=1)\n",
        "\n",
        "print (encoded_train_df.head())\n",
        "\n",
        "sample = encoded_train_df.iloc[0]['X']\n",
        "print ('Sample padded sequence:', sample)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print ('vocab_size:', vocab_size)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "encoded_train_df, encoded_val_df = train_test_split(encoded_train_df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   X  Y\n",
            "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  1\n",
            "1  [87, 88, 89, 12, 90, 91, 92, 93, 94, 95, 96, 9...  1\n",
            "2  [287, 288, 289, 203, 214, 215, 216, 290, 291, ...  1\n",
            "3  [282, 3, 356, 9, 116, 357, 283, 358, 9, 164, 3...  1\n",
            "4  [282, 3, 236, 9, 405, 163, 164, 283, 6, 406, 3...  1\n",
            "Sample padded sequence: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 12, 20, 21, 22, 23, 9, 24, 25, 26, 27, 28, 29, 30, 1, 2, 31, 3, 32, 33, 28, 34, 35, 3, 20, 9, 36, 28, 37, 38, 9, 39, 40, 41, 42, 43, 44, 45, 46, 47, 20, 48, 9, 49, 50, 9, 51, 52, 53, 54, 27, 50, 9, 55, 56, 57, 46, 40, 58, 59, 9, 60, 23, 61, 62, 63, 64, 28, 65, 66, 9, 17, 67, 68, 8, 2, 69, 70, 71, 72, 28, 73, 74, 50, 75, 20, 62, 76, 28, 1, 2, 77, 30, 78, 79, 50, 21, 80, 81, 30, 1, 2, 3, 82, 83, 84, 85, 30, 6, 86, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "vocab_size: 99401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3U5dChQ37NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "train_x = np.array(encoded_train_df['X'].tolist())\n",
        "train_y = np.array(encoded_train_df['Y'].tolist())\n",
        "val_x = np.array(encoded_val_df['X'].tolist())\n",
        "val_y = np.array(encoded_val_df['Y'].tolist())\n",
        "test_x = np.array(encoded_test_df['X'].tolist())\n",
        "test_y = np.array(encoded_test_df['Y'].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEWUucM337NY",
        "colab_type": "text"
      },
      "source": [
        "#### Creating and compiling the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOcLtPvf37NZ",
        "colab_type": "code",
        "outputId": "2b899b7c-0708-4ae1-a192-b7e8560132c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "models = [0] * 6\n",
        "models[0] = Sequential([\n",
        "    layers.Embedding(100000, 128, input_length=200),\n",
        "    layers.SimpleRNN(200, activation='tanh'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (models[0].summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 200, 128)          12800000  \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 200)               65800     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 12,866,001\n",
            "Trainable params: 12,866,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVSNaLs-37Nc",
        "colab_type": "code",
        "outputId": "216a8abc-b774-464b-9408-21b63b4db0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "models[1] = Sequential([\n",
        "    layers.Embedding(100000, 128, input_length=200),\n",
        "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (models[1].summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 200, 128)          12800000  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               263200    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 13,063,401\n",
            "Trainable params: 13,063,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsmiKgvp37Nf",
        "colab_type": "code",
        "outputId": "9f0e6a44-28d6-4793-ce8f-b8e923a94eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "models[2] = Sequential([\n",
        "    layers.Embedding(100000, 128, input_length=200),\n",
        "    layers.GRU(200, activation='relu', recurrent_activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (models[2].summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 200, 128)          12800000  \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 200)               197400    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 12,997,601\n",
            "Trainable params: 12,997,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOXDrng37Ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = keras.optimizers.Adam(1e-4)\n",
        "for i in range(3):\n",
        "    models[i].compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpyLDxg937Nl",
        "colab_type": "text"
      },
      "source": [
        "#### Initialising training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HBML5Zy37Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from tensorflow.keras.callbacks import EarlyStopping, History\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, \n",
        "                           verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
        "callbacks_list = [early_stop]\n",
        "hist_temp = 'keras_imdb_history{}.pkl'\n",
        "hist_names = [hist_temp.format(x) for x in range(1, 7)]\n",
        "template = 'keras_imdb_trained_model{}.h5'\n",
        "model_names = [template.format(x) for x in range(1, 7)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XIuSFMn37No",
        "colab_type": "code",
        "outputId": "8602de29-3930-4a66-be43-7ea05b2b5a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def save_history (hist_path, history):\n",
        "    print ('Saving history at', hist_path, flush=True)\n",
        "    file_object = open (hist_path, 'wb')\n",
        "    pickle.dump (history, file_object)\n",
        "    file_object.close()\n",
        "    \n",
        "def load_history (hist_path):\n",
        "    print ('Loading history from', hist_path, flush=True)\n",
        "    file_object = open(hist_path, 'rb')\n",
        "    history = pickle.load(file_object)\n",
        "    file_object.close()\n",
        "    return history\n",
        "\n",
        "def train_and_evaluate(model_id):\n",
        "    model = models[model_id]\n",
        "    model_path = os.path.join(save_dir, model_names[model_id])\n",
        "    hist_path = os.path.join(save_dir, hist_names[model_id])\n",
        "    if os.path.exists(hist_path):\n",
        "        assert (os.path.exists(model_path), 'Model must be saved at model_path')\n",
        "        history = load_history(hist_path)\n",
        "        print ('Trained model loaded from {}', model_path)\n",
        "        model = keras.models.load_model(model_path)\n",
        "        loss, accuracy = model.evaluate (test_x, test_y, verbose=0)\n",
        "        print ('Loss = {}, Accuracy = {}'.format(loss, accuracy))\n",
        "        return history, loss, accuracy\n",
        "        \n",
        "    history = model.fit(train_x, train_y, epochs=epochs, \n",
        "                         validation_data=(val_x, val_y), \n",
        "                         workers=4, shuffle=True, callbacks=callbacks_list)\n",
        "    \n",
        "    loss, accuracy = model.evaluate(test_x, test_y, verbose=0)\n",
        "    print ('Loss = {}, Accuracy = {}'.format(loss, accuracy))\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "    save_history(hist_path, history.history)\n",
        "    return history.history, loss, accuracy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-35-7a5f03d00746>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert (os.path.exists(model_path), 'Model must be saved at model_path')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23vkrl5637Nr",
        "colab_type": "code",
        "outputId": "dd5727c2-19f6-4ed3-e430-b14f03bce64b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_list = [0] * 6\n",
        "training_data = [0] * 6\n",
        "\n",
        "for i in range (3):\n",
        "    history, loss, accuracy = train_and_evaluate(i)\n",
        "    history_list[i] = history\n",
        "    data_dict = {'test_accuracy':accuracy, \n",
        "                 'train_accuracy':history['acc'][-1],\n",
        "                 'val_accuracy':history['val_acc'][-1]}\n",
        "    training_data[i] = data_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/150\n",
            "20000/20000 [==============================] - 67s 3ms/sample - loss: 0.6948 - acc: 0.5033 - val_loss: 0.6970 - val_acc: 0.5050\n",
            "Epoch 2/150\n",
            "20000/20000 [==============================] - 64s 3ms/sample - loss: 0.6751 - acc: 0.5688 - val_loss: 0.6993 - val_acc: 0.5066\n",
            "Epoch 3/150\n",
            "20000/20000 [==============================] - 65s 3ms/sample - loss: 0.6121 - acc: 0.6507 - val_loss: 0.7322 - val_acc: 0.5004\n",
            "Epoch 4/150\n",
            "20000/20000 [==============================] - 65s 3ms/sample - loss: 0.4828 - acc: 0.7450 - val_loss: 0.7872 - val_acc: 0.5056\n",
            "Epoch 5/150\n",
            "20000/20000 [==============================] - 66s 3ms/sample - loss: 0.3854 - acc: 0.7970 - val_loss: 0.8812 - val_acc: 0.5002\n",
            "Epoch 6/150\n",
            "20000/20000 [==============================] - 65s 3ms/sample - loss: 0.3141 - acc: 0.8419 - val_loss: 0.8798 - val_acc: 0.5128\n",
            "Epoch 7/150\n",
            "20000/20000 [==============================] - 66s 3ms/sample - loss: 0.2382 - acc: 0.8874 - val_loss: 0.9789 - val_acc: 0.5080\n",
            "Epoch 8/150\n",
            "20000/20000 [==============================] - 66s 3ms/sample - loss: 0.1641 - acc: 0.9336 - val_loss: 1.0135 - val_acc: 0.5026\n",
            "Epoch 9/150\n",
            "20000/20000 [==============================] - 67s 3ms/sample - loss: 0.1018 - acc: 0.9621 - val_loss: 1.0564 - val_acc: 0.5088\n",
            "Epoch 10/150\n",
            "20000/20000 [==============================] - 67s 3ms/sample - loss: 0.0577 - acc: 0.9823 - val_loss: 1.2956 - val_acc: 0.5002\n",
            "Epoch 11/150\n",
            "20000/20000 [==============================] - 66s 3ms/sample - loss: 0.0276 - acc: 0.9927 - val_loss: 1.3561 - val_acc: 0.5038\n",
            "Loss = 1.3529456232452393, Accuracy = 0.5081999897956848\n",
            "Saved trained model at /content/saved_models/keras_imdb_trained_model1.h5 \n",
            "Saving history at /content/saved_models/keras_imdb_history1.pkl\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/150\n",
            "20000/20000 [==============================] - 188s 9ms/sample - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5026\n",
            "Epoch 2/150\n",
            "20000/20000 [==============================] - 186s 9ms/sample - loss: 0.6931 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5006\n",
            "Epoch 3/150\n",
            "20000/20000 [==============================] - 188s 9ms/sample - loss: 0.6923 - acc: 0.5068 - val_loss: 0.6879 - val_acc: 0.5110\n",
            "Epoch 4/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.6458 - acc: 0.5982 - val_loss: 0.6035 - val_acc: 0.6858\n",
            "Epoch 5/150\n",
            "20000/20000 [==============================] - 186s 9ms/sample - loss: 0.5937 - acc: 0.7046 - val_loss: 0.6949 - val_acc: 0.5950\n",
            "Epoch 6/150\n",
            "20000/20000 [==============================] - 188s 9ms/sample - loss: 0.5633 - acc: 0.7323 - val_loss: 0.5780 - val_acc: 0.7336\n",
            "Epoch 7/150\n",
            "20000/20000 [==============================] - 188s 9ms/sample - loss: 0.5290 - acc: 0.7724 - val_loss: 0.5744 - val_acc: 0.7416\n",
            "Epoch 8/150\n",
            "20000/20000 [==============================] - 190s 10ms/sample - loss: 0.5005 - acc: 0.7997 - val_loss: 0.5434 - val_acc: 0.7490\n",
            "Epoch 9/150\n",
            "20000/20000 [==============================] - 190s 10ms/sample - loss: 0.4754 - acc: 0.8177 - val_loss: 0.5495 - val_acc: 0.7526\n",
            "Epoch 10/150\n",
            "20000/20000 [==============================] - 192s 10ms/sample - loss: 0.4765 - acc: 0.8093 - val_loss: 0.5518 - val_acc: 0.7746\n",
            "Epoch 11/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.4941 - acc: 0.8026 - val_loss: 0.5279 - val_acc: 0.7830\n",
            "Epoch 12/150\n",
            "20000/20000 [==============================] - 189s 9ms/sample - loss: 0.4375 - acc: 0.8512 - val_loss: 0.5098 - val_acc: 0.7818\n",
            "Epoch 13/150\n",
            "20000/20000 [==============================] - 193s 10ms/sample - loss: 0.3649 - acc: 0.8831 - val_loss: 0.5128 - val_acc: 0.7886\n",
            "Epoch 14/150\n",
            "20000/20000 [==============================] - 190s 10ms/sample - loss: 0.3316 - acc: 0.8974 - val_loss: 0.4818 - val_acc: 0.8092\n",
            "Epoch 15/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.3023 - acc: 0.9111 - val_loss: 0.4968 - val_acc: 0.8132\n",
            "Epoch 16/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.2800 - acc: 0.9212 - val_loss: 0.4803 - val_acc: 0.8220\n",
            "Epoch 17/150\n",
            "20000/20000 [==============================] - 186s 9ms/sample - loss: 0.2533 - acc: 0.9291 - val_loss: 0.4735 - val_acc: 0.8254\n",
            "Epoch 18/150\n",
            "20000/20000 [==============================] - 187s 9ms/sample - loss: 0.2231 - acc: 0.9398 - val_loss: 0.5066 - val_acc: 0.8270\n",
            "Epoch 19/150\n",
            "20000/20000 [==============================] - 186s 9ms/sample - loss: 0.1743 - acc: 0.9489 - val_loss: 0.4287 - val_acc: 0.8290\n",
            "Epoch 20/150\n",
            "20000/20000 [==============================] - 186s 9ms/sample - loss: 0.1451 - acc: 0.9581 - val_loss: 0.4683 - val_acc: 0.8352\n",
            "Epoch 21/150\n",
            "20000/20000 [==============================] - 184s 9ms/sample - loss: 0.1278 - acc: 0.9633 - val_loss: 0.4939 - val_acc: 0.8332\n",
            "Epoch 22/150\n",
            "20000/20000 [==============================] - 186s 9ms/sample - loss: 0.1071 - acc: 0.9700 - val_loss: 0.4798 - val_acc: 0.8360\n",
            "Epoch 23/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.0898 - acc: 0.9750 - val_loss: 0.5223 - val_acc: 0.8356\n",
            "Epoch 24/150\n",
            "20000/20000 [==============================] - 189s 9ms/sample - loss: 0.0845 - acc: 0.9763 - val_loss: 0.6137 - val_acc: 0.8358\n",
            "Epoch 25/150\n",
            "20000/20000 [==============================] - 189s 9ms/sample - loss: 0.0689 - acc: 0.9821 - val_loss: 0.5552 - val_acc: 0.8392\n",
            "Epoch 26/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.0607 - acc: 0.9850 - val_loss: 0.5079 - val_acc: 0.8302\n",
            "Epoch 27/150\n",
            "20000/20000 [==============================] - 190s 9ms/sample - loss: 0.0511 - acc: 0.9875 - val_loss: 0.5531 - val_acc: 0.8386\n",
            "Epoch 28/150\n",
            "20000/20000 [==============================] - 191s 10ms/sample - loss: 0.0592 - acc: 0.9850 - val_loss: 0.5848 - val_acc: 0.8442\n",
            "Epoch 29/150\n",
            "20000/20000 [==============================] - 192s 10ms/sample - loss: 0.0396 - acc: 0.9904 - val_loss: 0.5679 - val_acc: 0.8344\n",
            "Loss = 0.6777673495340347, Accuracy = 0.8055599927902222\n",
            "Saved trained model at /content/saved_models/keras_imdb_trained_model2.h5 \n",
            "Saving history at /content/saved_models/keras_imdb_history2.pkl\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/150\n",
            "20000/20000 [==============================] - 177s 9ms/sample - loss: 0.6931 - acc: 0.5009 - val_loss: 0.6930 - val_acc: 0.5096\n",
            "Epoch 2/150\n",
            "20000/20000 [==============================] - 176s 9ms/sample - loss: 0.6885 - acc: 0.5475 - val_loss: 0.6926 - val_acc: 0.5164\n",
            "Epoch 3/150\n",
            "20000/20000 [==============================] - 175s 9ms/sample - loss: nan - acc: 0.5030 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 4/150\n",
            "20000/20000 [==============================] - 176s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 5/150\n",
            "20000/20000 [==============================] - 178s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 6/150\n",
            "20000/20000 [==============================] - 179s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 7/150\n",
            "20000/20000 [==============================] - 178s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 8/150\n",
            "20000/20000 [==============================] - 177s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 9/150\n",
            "20000/20000 [==============================] - 180s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 10/150\n",
            "20000/20000 [==============================] - 181s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 11/150\n",
            "20000/20000 [==============================] - 179s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Epoch 12/150\n",
            "20000/20000 [==============================] - 179s 9ms/sample - loss: nan - acc: 0.4994 - val_loss: nan - val_acc: 0.5026\n",
            "Loss = nan, Accuracy = 0.5\n",
            "Saved trained model at /content/saved_models/keras_imdb_trained_model3.h5 \n",
            "Saving history at /content/saved_models/keras_imdb_history3.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vbc9ao4HyJ_",
        "colab_type": "text"
      },
      "source": [
        "#### Clearly Model 2 gives the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0lyMcpk37Nu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "39a64fa8-690c-4d8b-d112-afaebc436021"
      },
      "source": [
        "models[3] = Sequential([\n",
        "    layers.Embedding(100000, 128, input_length=200),\n",
        "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh', return_sequences=True),\n",
        "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (models[3].summary())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 200, 128)          12800000  \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 200, 200)          263200    \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 200)               320800    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 13,384,201\n",
            "Trainable params: 13,384,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cUWjlje37Nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "23dcaa1d-ba45-4a4a-b698-40f408355901"
      },
      "source": [
        "models[4] = Sequential([\n",
        "    layers.Embedding(100000, 128, input_length=200),\n",
        "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh', return_sequences=True),\n",
        "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh', return_sequences=True),\n",
        "    layers.LSTM(200, activation='tanh', recurrent_activation='tanh'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "print (models[4].summary())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 200, 128)          12800000  \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 200, 200)          263200    \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 200, 200)          320800    \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 200)               320800    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 13,705,001\n",
            "Trainable params: 13,705,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmNBBJNrHP-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = keras.optimizers.Adam(1e-4)\n",
        "for i in range(3, 5):\n",
        "    models[i].compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXeDYkttHQBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c38d13f2-69ef-4568-f927-6478acbd0593"
      },
      "source": [
        "for i in range (3, 5):\n",
        "    history, loss, accuracy = train_and_evaluate(i)\n",
        "    history_list[i] = history\n",
        "    data_dict = {'test_accuracy':accuracy, \n",
        "                 'train_accuracy':history['acc'][-1],\n",
        "                 'val_accuracy':history['val_acc'][-1]}\n",
        "    training_data[i] = data_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/150\n",
            "20000/20000 [==============================] - 385s 19ms/sample - loss: 0.6932 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.4974\n",
            "Epoch 2/150\n",
            "20000/20000 [==============================] - 380s 19ms/sample - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.4974\n",
            "Epoch 3/150\n",
            "19008/20000 [===========================>..] - ETA: 17s - loss: 0.6931 - acc: 0.4988"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBwtleL2MYb3",
        "colab_type": "text"
      },
      "source": [
        "Model 2 still outperforms the other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGXkVupsLhfT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Tabulating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcvGpoutHP8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (5):\n",
        "    print ('Model:', i)\n",
        "    print (training_data[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxtwrgIkLk3n",
        "colab_type": "text"
      },
      "source": [
        "#### Plotting the training loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_yUNkNuIZ0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(4, 10))\n",
        "for i in range (0, 5):\n",
        "    plt.subplot(9, 2, 2*i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.plot(range(1, len(history_list[i]['acc']) + 1), history_list[i]['acc'])\n",
        "    plt.ylabel('Accuracy', fontsize=10)\n",
        "    plt.xlabel('Model ' + str(i + 1), fontsize=10)\n",
        "    plt.subplot(9, 2, 2*i+2)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.plot(range(1, len(history_list[i]['loss']) + 1), history_list[i]['loss'])\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Model ' + str(i + 1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBfwaVl_IZ6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !cp \"./saved_models\" -r \"gdrive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}